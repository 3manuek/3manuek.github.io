<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Playing with Postgres and Kafka. | tr3sma</title><meta name=description content><meta property="og:title" content="Playing with Postgres and Kafka."><meta property="og:description" content="Apache Kafka and Postgres: Transaction and reporting capabilities Apache Kafka is a well known distributed streaming platform for data processing and consistent messaging. It allows you to consistently centralize data streams for several purposes by consuming and producing them.
One of the examples of a nice implementation, is the Mozilla&rsquo;s Data pipeline implementation, particularly as it shows Kafka as an entry point of the data flow. This allows you to plug new data stores bellow its stream, making it easy to use different data store formats ( such as DRBMS or Document, etc."><meta property="og:type" content="article"><meta property="og:url" content="https://tr3s.ma/blog/2017-02/kafka-and-copy-in-pg/"><meta property="og:image" content="https://tr3s.ma/blog/assets/thumbnail_db.png"><meta property="og:image" content="https://tr3s.ma/blog/assets/tachyons-logo-script-feature.png"><meta property="article:published_time" content="2017-02-28T00:00:00+00:00"><meta property="article:modified_time" content="2017-02-28T00:00:00+00:00"><meta itemprop=name content="Playing with Postgres and Kafka."><meta itemprop=description content="Apache Kafka and Postgres: Transaction and reporting capabilities Apache Kafka is a well known distributed streaming platform for data processing and consistent messaging. It allows you to consistently centralize data streams for several purposes by consuming and producing them.
One of the examples of a nice implementation, is the Mozilla&rsquo;s Data pipeline implementation, particularly as it shows Kafka as an entry point of the data flow. This allows you to plug new data stores bellow its stream, making it easy to use different data store formats ( such as DRBMS or Document, etc."><meta itemprop=datePublished content="2017-02-28T00:00:00+00:00"><meta itemprop=dateModified content="2017-02-28T00:00:00+00:00"><meta itemprop=wordCount content="1029"><meta itemprop=image content="https://tr3s.ma/blog/assets/thumbnail_db.png"><meta itemprop=image content="https://tr3s.ma/blog/assets/tachyons-logo-script-feature.png"><meta itemprop=keywords content="hugo-site,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tr3s.ma/blog/assets/thumbnail_db.png"><meta name=twitter:title content="Playing with Postgres and Kafka."><meta name=twitter:description content="Apache Kafka and Postgres: Transaction and reporting capabilities Apache Kafka is a well known distributed streaming platform for data processing and consistent messaging. It allows you to consistently centralize data streams for several purposes by consuming and producing them.
One of the examples of a nice implementation, is the Mozilla&rsquo;s Data pipeline implementation, particularly as it shows Kafka as an entry point of the data flow. This allows you to plug new data stores bellow its stream, making it easy to use different data store formats ( such as DRBMS or Document, etc."><meta name=twitter:site content="@3manuek"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','G-GY0KSNNSTL','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><!--[if IE]><script src=//html5shiv.googlecode.com/svn/trunk/html5.js></script><![endif]--><link rel="shortcut icon" href="/img/favicon.ico?v=0" type=image/x-icon><link rel=icon href="/img/favicon.ico?v=0" type=image/x-icon><link rel=stylesheet href=https://tr3s.ma/style.main.min.af051a891b2af29b5e832059f2d65564f8acece3291dcaca54df6ef06e3e084e.css integrity="sha256-rwUaiRsq8ptegyBZ8tZVZPis7OMpHcrKVN9u8G4+CE4=" media=screen></head><body><div class="grid-container single"><header class="site-header pa4 bb b--transparent" role=banner><nav class="site-nav db dt-l w-100" role=nav><a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href=https://tr3s.ma title=Home><img src=/img/firma.png class="dib db-l h2 w-auto" alt=tr3sma></a><div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l"><a class="link f6 f5-l dib pv1 ph2" href=/ title="Home Page">Home</a>
<a class="link f6 f5-l dib pv1 ph2" href=/about/ title="About tr3sma">About</a>
<a class="link f6 f5-l dib pv1 ph2" href=/blog/ title=Blog>Blog</a>
<a class="link f6 f5-l dib pv1 ph2" href=/contact/ title="Contact Form">Contact</a></div></nav></header><main class="page-main pa4" role=main><section class="page-content mw7 center"><article class="post-content pa0 ph4-l"><header class=post-header><h1 class="f2 f1-m f-subheadline-l fw5-ns mv4 lh-solid tracked-tight">Playing with Postgres and Kafka.</h1><h2 class="f7 fw7 measure mv0 ttu tracked">Using the dirty way.</h2><p class="f6 measure lh-copy mv1">By 3manuek in <a href=https://tr3s.ma/categories/theme-features>Theme Features</a></p><p class="f7 db mv0 ttu">February 28, 2017</p></header><section class="post-body pt5 pb4"><h2 id=apache-kafka-and-postgres-transaction-and-reporting-capabilities>Apache Kafka and Postgres: Transaction and reporting capabilities</h2><p><a href=https://kafka.apache.org/>Apache Kafka</a> is a well known distributed streaming platform for data processing
and consistent messaging. It allows you to consistently centralize data streams for
several purposes by consuming and producing them.</p><p>One of the examples of a nice implementation, is the <a href=https://robertovitillo.com/2017/01/23/an-overview-of-mozillas-data-pipeline/>Mozilla&rsquo;s Data pipeline implementation</a>,
particularly as it shows Kafka as an entry point of the data flow. This allows you to plug
new data stores bellow its stream, making it easy to use different data store formats (
such as DRBMS or Document, etc. ) for retrieving and writing data efficiently.</p><p><a href=https://www.confluent.io/blog/bottled-water-real-time-integration-of-postgresql-and-kafka/>Postgres Bottled water</a> is a different approach that deserves a mention. In this
case, Postgres instances are the producers, brokers consume the streams and keep the message
store available for any action. The advantage here is the well known Postgres&rsquo;
ACID capabilities, combined with advanced SQL features. This project is an extension,
meaning that is possible to use new upcoming Postgres features easily portable.</p><p>It is possible also, to consume and produce data to a broker by using a new feature
that extended the COPY tool for executing shell commands for input/output operations.
A nice highlight of this feature can be read <a href=http://paquier.xyz/postgresql-2/postgres-9-6-feature-highlight-copy-dml-statements/>here</a>.</p><p><img src=/blog/assets/2017-02/kafka.jpg alt=kafka></p><h2 id=kafkacat-and-librdkafka>kafkacat and librdkafka</h2><p><a href=https://github.com/edenhill/kafkacat>kafkacat</a> is a tool based on the same author&rsquo;s library <a href=https://github.com/edenhill/librdkafka>librdkafka</a> which
does exactly what its name says: produce and consume from a Kafka broker like <code>cat</code>
command.</p><h2 id=producing-to-kafka-broker>Producing to Kafka broker</h2><p>Producing fake data to the Kafka broker, composed by <code>key</code> and <code>payload</code>:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>
<span style=color:#60a0b0;font-style:italic># Random text</span>
randtext<span style=color:#666>(</span><span style=color:#666>)</span> <span style=color:#666>{</span>cat /dev/urandom | tr -dc <span style=color:#4070a0>&#39;a-zA-Z0-9&#39;</span> | fold -w <span style=color:#40a070>32</span> | head -n 1<span style=color:#666>}</span>
<span style=color:#007020;font-weight:700>while</span> <span style=color:#666>(</span><span style=color:#007020>true</span><span style=color:#666>)</span> ;
  <span style=color:#007020;font-weight:700>do</span>
    <span style=color:#007020;font-weight:700>for</span> i in <span style=color:#007020;font-weight:700>$(</span>seq <span style=color:#40a070>1</span> 50<span style=color:#007020;font-weight:700>)</span>  
      <span style=color:#007020;font-weight:700>do</span> <span style=color:#007020>echo</span> <span style=color:#4070a0>&#34;</span><span style=color:#007020;font-weight:700>$(</span>uuidgen<span style=color:#007020;font-weight:700>)</span><span style=color:#4070a0>;</span><span style=color:#007020;font-weight:700>$(</span>randtext<span style=color:#007020;font-weight:700>)</span><span style=color:#4070a0>&#34;</span>
     <span style=color:#007020;font-weight:700>done</span>  | kafkacat -P -b localhost:9092 -qe -K <span style=color:#4070a0>&#39;;&#39;</span> -t PGSHARD
     sleep <span style=color:#40a070>10</span>
  <span style=color:#007020;font-weight:700>done</span>
</code></pre></td></tr></table></div></div><p><code>-K</code> option defines the delimiter between the <em>key</em> and the <em>payload</em>, <code>-t</code> defines
the topic you want to produce for. Originally, this topic has been created with 3
partitions (0-2), which will allow us to consume data in different channels, opening
the door for parallelization.</p><p><em>Keys</em> aren&rsquo;t mandatory when producing to a broker, and actually for certain solutions
you can omit it.</p><h2 id=consuming-and-producing-inside-a-postgres-instance>Consuming and Producing inside a Postgres instance</h2><p>The general syntax will be something close as:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=color:#007020;font-weight:700>COPY</span> main(group_id,payload)
  <span style=color:#007020;font-weight:700>FROM</span> PROGRAM
  <span style=color:#4070a0>&#39;</span><span style=color:#4070a0>kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 0 | awk </span><span style=color:#4070a0>&#39;&#39;</span><span style=color:#4070a0>{print &#34;P0\t&#34; $0 }</span><span style=color:#4070a0>&#39;&#39;</span><span style=color:#4070a0> </span><span style=color:#4070a0>&#39;</span>;
</code></pre></td></tr></table></div></div><p>Code piping to an <code>awk</code> is not strictly necessary and it is only for showing the
flexibility of the feature. When using the option <code>-J</code>, the output will be printed
in json format, containing all the message information, including partition, key and
message.</p><p><code>-c</code> option will limit the amount of rows in the operation. As COPY is transactional,
be aware that the higher is the amount of rows, the larger will be the transaction and
COMMIT times will be affected.</p><h3 id=consuming-topics-incrementally>Consuming topics incrementally</h3><p>Consuming the topic partitions from the <code>beginning</code> and setting a limit of <code>100</code>
documents is easy as:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>bin/psql -p7777 -Upostgres master <span style=color:#4070a0>&lt;&lt;EOF
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 0 | awk &#39;&#39;{print &#34;P0\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 1 | awk &#39;&#39;{print &#34;P1\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 2 | awk &#39;&#39;{print &#34;P2\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>EOF</span>
</code></pre></td></tr></table></div></div><p>And then using <code>stored</code>, in order to consume from the last offset consumed by the
consumer on the group:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>bin/psql -p7777 -Upostgres master <span style=color:#4070a0>&lt;&lt;EOF
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 0 | awk &#39;&#39;{print &#34;P0\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 1 | awk &#39;&#39;{print &#34;P1\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>COPY main(group_id,payload) FROM PROGRAM &#39;kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 2 | awk &#39;&#39;{print &#34;P2\t&#34; \$0 }&#39;&#39; &#39;;
</span><span style=color:#4070a0>EOF</span>
</code></pre></td></tr></table></div></div><p>Each COPY line, can be executed in parallel in different Postgres instances, making
this approach flexible and easy scalable across a board of servers.</p><p>This is not entirely consistent, as once the offset is consumed, will be marked
in the broker, wether if transaction fails at Postgres side can potentially lead
to data missing.</p><h3 id=producing-messages-out-the-postgres-instances>Producing messages out the Postgres instances</h3><p>The same way is possible to consume changes, it is possible to do the same for producing
data to the broker. This can be incredibly useful for micro aggregations, done over the
consumed raw data from the broker.</p><p>The bellow example shows how to run a simple query with a very simplistic aggregation
and publish it in JSON format to the broker:</p><pre><code>master=# COPY (select row_to_json(row(now() ,group_id , count(*))) from main group by group_id)
         TO PROGRAM 'kafkacat -P -b localhost:9092 -qe  -t AGGREGATIONS';
COPY 3
</code></pre><p>If you have a farm of servers and want to search the topic contents using a key,
you can do the following tweak:</p><pre><code>COPY (select inet_server_addr() || ';', row_to_json(row(now() ,group_id , count(*))) from main group by group_id)
   TO PROGRAM 'kafkacat -P -K '';'' -b localhost:9092 -qe  -t AGGREGATIONS';
</code></pre><p>This is how the published payloads look like (without <em>key</em>):</p><pre><code>➜  PG10 kafkacat -C -b localhost:9092 -qeJ -t AGGREGATIONS -X group.id=1  -o beginning
{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:0,&quot;key&quot;:&quot;&quot;,&quot;payload&quot;:&quot;{\&quot;f1\&quot;:\&quot;2017-02-24T12:34:13.711732-03:00\&quot;,\&quot;f2\&quot;:\&quot;P1\&quot;,\&quot;f3\&quot;:172}&quot;}
{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:1,&quot;key&quot;:&quot;&quot;,&quot;payload&quot;:&quot;{\&quot;f1\&quot;:\&quot;2017-02-24T12:34:13.711732-03:00\&quot;,\&quot;f2\&quot;:\&quot;P0\&quot;,\&quot;f3\&quot;:140}&quot;}
{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:2,&quot;key&quot;:&quot;&quot;,&quot;payload&quot;:&quot;{\&quot;f1\&quot;:\&quot;2017-02-24T12:34:13.711732-03:00\&quot;,\&quot;f2\&quot;:\&quot;P2\&quot;,\&quot;f3\&quot;:155}&quot;}
</code></pre><p>&mldr; and with <em>key</em>:</p><pre><code>{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:3,&quot;key&quot;:&quot;127.0.0.1/32&quot;,&quot;payload&quot;:&quot;\t{\&quot;f1\&quot;:\&quot;2017-02-24T12:40:39.017644-03:00\&quot;,\&quot;f2\&quot;:\&quot;P1\&quot;,\&quot;f3\&quot;:733}&quot;}
{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:4,&quot;key&quot;:&quot;127.0.0.1/32&quot;,&quot;payload&quot;:&quot;\t{\&quot;f1\&quot;:\&quot;2017-02-24T12:40:39.017644-03:00\&quot;,\&quot;f2\&quot;:\&quot;P0\&quot;,\&quot;f3\&quot;:994}&quot;}
{&quot;topic&quot;:&quot;AGGREGATIONS&quot;,&quot;partition&quot;:0,&quot;offset&quot;:5,&quot;key&quot;:&quot;127.0.0.1/32&quot;,&quot;payload&quot;:&quot;\t{\&quot;f1\&quot;:\&quot;2017-02-24T12:40:39.017644-03:00\&quot;,\&quot;f2\&quot;:\&quot;P2\&quot;,\&quot;f3\&quot;:716}&quot;}
</code></pre><h2 id=basic-topic-manipulation>Basic topic manipulation</h2><p>If you are new into Kafka, you will find useful to count with a few command examples
to play with your local broker.</p><p>Starting everything:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>bin/zookeeper-server-start.sh config/zookeeper.properties 2&gt; zookeper.log &amp;
bin/kafka-server-start.sh config/server.properties 2&gt; kafka.log &amp;
</code></pre></td></tr></table></div></div><p>Creating topics and others:</p><div class=highlight><div style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>bin/kafka-topics.sh --list --zookeeper localhost:2181
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span style=color:#40a070>1</span> --partitions <span style=color:#40a070>3</span> --topic PGSHARD
bin/kafka-topics.sh --delete  --zookeeper localhost:2181 --topic PGSHARD
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span style=color:#40a070>1</span> --partitions <span style=color:#40a070>1</span> --topic AGGREGATIONS
bin/kafka-topics.sh --delete  --zookeeper localhost:2181 --topic AGGREGATIONS
</code></pre></td></tr></table></div></div><blockquote><p>NOTE: For deleting topics, you need to enable the <code>delete.topic.enable=true</code> in
server.properties file.</p></blockquote><p>Hope you find this useful!</p><details closed class="f6 fw7 input-reset"><dl class="f6 lh-copy"><dt class=fw7>Posted on:</dt><dd class="fw5 ml0">February 28, 2017</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Length:</dt><dd class="fw5 ml0">5 minute read, 1029 words</dd></dl><dl class="f6 lh-copy"><dt class=fw7>Categories:</dt><dd class="fw5 ml0"><a href=https://tr3s.ma/categories/theme-features>Theme Features</a></dd></dl><dl class="f6 lh-copy"><dt class=fw7>Series:</dt><dd class="fw5 ml0"><a href=https://tr3s.ma/series/getting-started>Getting Started</a></dd></dl><dl class="f6 lh-copy"><dt class=fw7>Tags:</dt><dd class="fw5 ml0"><a href=https://tr3s.ma/tags/hugo-site>hugo-site</a></dd></dl><dl class="f6 lh-copy"><dt class=fw7>See Also:</dt><dd class="fw5 ml0"><a href=/blog/2020-01/openlabs/>Open Labs</a></dd><dd class="fw5 ml0"><a href=/blog/2019-07/terraformwithpostgres/>What are the perks of using Postgres as a Terraform backend?</a></dd><dd class="fw5 ml0"><a href=/blog/2019-07/ansiblekubernetes/>Ansible and Kubernetes</a></dd></dl></details></section><footer class=post-footer><div class="post-pagination dt w-100 mt4 mb2"><a class="prev dtc pr2 tl v-top fw6" href=https://tr3s.ma/blog/2017-02/postgres-10-logrep-y-part/>&larr; Highlighting Postgres 10 new features: Logical Replication and Partitioning.</a>
<a class="next dtc pl2 tr v-top fw6" href=https://tr3s.ma/blog/2017-03/simple-manual-sharding/>Simple and manual sharding on PostgreSQL. &rarr;</a></div></footer></article></section></main><footer class="site-footer pa4 bt b--transparent" role=contentinfo><nav class="db dt-l w-100"><p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">&copy; 2022 tr3sma, Madrid, Spain<br></p><div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0"><a class="dib pv1 ph2 link" href=/index.xml title="RSS Feed">RSS</a></div></nav></footer></div></body></html>