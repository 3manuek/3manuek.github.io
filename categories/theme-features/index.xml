<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Theme Features on tr3sma</title><link>/categories/theme-features/</link><description>Recent content in Theme Features on tr3sma</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 20 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="/categories/theme-features/index.xml" rel="self" type="application/rss+xml"/><item><title>Open Labs</title><link>/blog/2020-01/openlabs/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>/blog/2020-01/openlabs/</guid><description>Database Laboratories During the last years, I&amp;rsquo;ve been working on several open labs for Database trainings and webinars for OnGres and CanalDBA community. Some of these laboratories may look out-of-date, but they should commit their purpose for references and examples on how to setup them with few lines of code and with the most popular technologies.
If you are willing to start with Ansible, Terraform, Docker and others for Databases, you may found this resources interesting for you.</description></item><item><title>What are the perks of using Postgres as a Terraform backend?</title><link>/blog/2019-07/terraformwithpostgres/</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid>/blog/2019-07/terraformwithpostgres/</guid><description>Before starting the container This article is not an introductory explanation of docker,however it&amp;rsquo;s scope if for docker&amp;rsquo;s beginners. You can consider it as an extension of the well documented Percona docker hub doc. For the source code of the image, the repository is at github.
Here is the all what you need to do for start:
1 2 3 docker run --name percona57 \ -e MYSQL_ROOT_PASSWORD=&amp;lt;a_password&amp;gt; \ -d percona:5.</description></item><item><title>Ansible and Kubernetes</title><link>/blog/2019-07/ansiblekubernetes/</link><pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate><guid>/blog/2019-07/ansiblekubernetes/</guid><description>Install minikube or similar [](https://github.com/kubernetes/minikube/releases)
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.2.0/minikube-darwin-amd64 &amp;amp;&amp;amp;\ chmod +x minikube &amp;amp;&amp;amp; sudo cp minikube /usr/local/bin/ &amp;amp;&amp;amp; rm minikube Getting the token Access Kubernetes API
Ansible k8s [](https://docs.ansible.com/ansible/latest/modules/k8s_module.html#k8s-raw-module)
K8S_AUTH_API_KEY in environment.
Getting facts with k8s_fact [](https://docs.ansible.com/ansible/latest/modules/k8s_facts_module.html#k8s-facts-module)</description></item><item><title>Google Cloud TCP Internal Load Balancing with HTTP Health Checks in Terraform for stateful services</title><link>/blog/2019-06/ilb-internal-gcp/</link><pubDate>Wed, 19 Jun 2019 00:00:00 +0000</pubDate><guid>/blog/2019-06/ilb-internal-gcp/</guid><description>Components and HCL code snippets to plug in iLB with your stateful services through HTTP API and TCP protocol.</description></item><item><title>Clickhouse sampling on MergeTree engine.</title><link>/blog/2019-06/clickhouse-sampling-data/</link><pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate><guid>/blog/2019-06/clickhouse-sampling-data/</guid><description>Why sampling is important and what you need to be aware of? When dealing with very large amount of data, you probably want to run your queries only for a smaller dataset in your current tables. Specially if your dataset is not fitting in RAM.
MergeTree is the first and more advanced engine on Clickhouse that you want to try. It supports indexing by Primary Key and it is mandatory to have a column of Date type (used for automatic partitioning).</description></item><item><title>Import data from Redshift into Clickhouse in a single command.</title><link>/blog/2017-03/import-data-to-ch-from-rs/</link><pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate><guid>/blog/2017-03/import-data-to-ch-from-rs/</guid><description>Scope If you heard about Clickhouse and you are wondering how to test with your residing data in Redshift, here is a command that will show you a few tips to make you speed up.
Update (July 4th): There is a serie of posts about Clickhouse vs Redshift comparisons, the first post is this one.
The standard wat to move your data out of Redshift is by using UNLOAD command, which pushes the output into S3 files.</description></item><item><title>postgres_fdw estimated overhead</title><link>/blog/2017-03/fdw-overhead/</link><pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate><guid>/blog/2017-03/fdw-overhead/</guid><description>Concept In the current concept, we are going to combine Foreign tables inheritance with the postgres_fdw extension, both being already available features since 9.5 version.
Cross-node partitioning allows a better data locality and a more scalable model than keeping local partitions. Being said, the data will be split into several nodes and organized using a particular key, which will determine in which shard data will be allocated. For the current POC, we are going to specify the shardKey , which is a simple char(2) type.</description></item><item><title>Simple and manual sharding on PostgreSQL.</title><link>/blog/2017-03/simple-manual-sharding/</link><pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate><guid>/blog/2017-03/simple-manual-sharding/</guid><description>Before starting the container This article is not an introductory explanation of docker,however it&amp;rsquo;s scope if for docker&amp;rsquo;s beginners. You can consider it as an extension of the well documented Percona docker hub doc. For the source code of the image, the repository is at github.
Here is the all what you need to do for start:
1 docker run --name percona57 -e MYSQL_ROOT_PASSWORD=&amp;lt;a_password&amp;gt; -d percona:5.7 For checking the container status log, you can execute docker logs percona57.</description></item><item><title>Playing with Postgres and Kafka.</title><link>/blog/2017-02/kafka-and-copy-in-pg/</link><pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate><guid>/blog/2017-02/kafka-and-copy-in-pg/</guid><description>Apache Kafka and Postgres: Transaction and reporting capabilities Apache Kafka is a well known distributed streaming platform for data processing and consistent messaging. It allows you to consistently centralize data streams for several purposes by consuming and producing them.
One of the examples of a nice implementation, is the Mozilla&amp;rsquo;s Data pipeline implementation, particularly as it shows Kafka as an entry point of the data flow. This allows you to plug new data stores bellow its stream, making it easy to use different data store formats ( such as DRBMS or Document, etc.</description></item><item><title>Highlighting Postgres 10 new features: Logical Replication and Partitioning.</title><link>/blog/2017-02/postgres-10-logrep-y-part/</link><pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate><guid>/blog/2017-02/postgres-10-logrep-y-part/</guid><description>Heya! I this article we are going to explore two of the major features commited in the upcoming PostgreSQL release: Logical Replication and Partitioning. Needeless to say that these features aren&amp;rsquo;t yet available in the stable release, so they are prune to change or extended.
Advertising warning! The current article is just a sneak peak of the upcoming talk Demystifying Logical Replication on PostgreSQL at Percona Live Santa Clara 2017.</description></item><item><title>binlogtop</title><link>/blog/2017-01/binlogtop/</link><pubDate>Mon, 23 Jan 2017 00:00:00 +0000</pubDate><guid>/blog/2017-01/binlogtop/</guid><description>Currently binlogtop is in status WIP and very alpha state, although there is a python script doing sort of the same thing is: binlogEventStats.
Hey! Replication is delaying, don&amp;rsquo;t know what&amp;rsquo;s happening. binlogtop just do a real-time streaming statistics.
What has inside?
golang If you want to indicate issues, go for it at report issues.</description></item><item><title>MyRocks Views</title><link>/blog/2017-01/myrocks-views/</link><pubDate>Sat, 21 Jan 2017 00:00:00 +0000</pubDate><guid>/blog/2017-01/myrocks-views/</guid><description>MyRocks is an storage engine available also in MongoDB, focused on performance and space saving. It is a LSM tree, with Bloom filtering for unique keys, providing steady performance in limited amount of cache. Installing can be done through a 5.6 fork, repository here.
Installing is easy as importing the sql file into your database.
Repository can be found here.
What has inside?
SQL</description></item><item><title>Go-Plus and Atom GOPATH fix</title><link>/blog/2016-10/goplus-and-atom-gopath-fix-howto/</link><pubDate>Wed, 05 Oct 2016 00:00:00 +0000</pubDate><guid>/blog/2016-10/goplus-and-atom-gopath-fix-howto/</guid><description>The background Golang is an awesome language, but I found it pretty unstable within the environment variables (at least in macOS Sierra/El Capitan). gvm is your friend btw, and it helped me to fix some of the issues by installing the latest release candidate of the 1.7.1 series.
Keep in mind that if you want to upgrade your macOS to Sierra, you&amp;rsquo;ll need to backup all of your environment variables and reinstall gvm.</description></item><item><title>PostgreSQL RDS pg-stat-ramdisk-size new feature and its calculations</title><link>/blog/2016-09/rds-pg-stat-ramdisk-size-on-rds/</link><pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate><guid>/blog/2016-09/rds-pg-stat-ramdisk-size-on-rds/</guid><description>IMPORTANT NOTE
This has been already addressed in PostgreSQL core, but this option
is still available in RDS.
What does it change and why is so important? Tracking databases and not just tables counters in Postgres isn&amp;rsquo;t cheap, but since some time ago there were workarounds involving the setup of a ramdisk to place the directory pointed by stat_temp_directory GUC variable. That directory places a global.</description></item><item><title>HOWTO Percona Server with Docker for DBAs</title><link>/blog/2016-07/percona-docker/</link><pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate><guid>/blog/2016-07/percona-docker/</guid><description>Before starting the container This article is not an introductory explanation of docker,however it&amp;rsquo;s scope if for docker&amp;rsquo;s beginners. You can consider it as an extension of the well documented Percona docker hub doc. For the source code of the image, the repository is at github.
Here is the all what you need to do for start:
1 docker run --name percona57 -e MYSQL_ROOT_PASSWORD=&amp;lt;a_password&amp;gt; -d percona:5.7 For checking the container status log, you can execute docker logs percona57.</description></item><item><title>Random data for PostgreSQL</title><link>/blog/2016-05/random-databases-postgres/</link><pubDate>Sun, 01 May 2016 00:00:00 +0000</pubDate><guid>/blog/2016-05/random-databases-postgres/</guid><description>Usually, you want to test code, try new features or just need decent content when generating random data. Random databases for Postgres addresses this need by simple SQL scripts that can be entirely generated inside your database.
It isn&amp;rsquo;t a sophisticated repository, although it is practical for quick usage.
What has inside?
Pure SQL If you want to indicate issues, go for it at report issues.</description></item><item><title>MySQL 5.7 InnoDB's Full Text Search overview.</title><link>/blog/2016-04/fts-in-mysql-5-7-with-innodb/</link><pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate><guid>/blog/2016-04/fts-in-mysql-5-7-with-innodb/</guid><description>Main application: InnoDB&amp;rsquo;s Full Text Search overview (with Shiny/R)
Credits Author: Emanuel Calvo
Company: Pythian
Thanks to Valerie Parham-Thompson @ Pythian and Daniel Prince @ Oracle.
Repository available at Github.
For the whole article and the Shinyapp application is available here.
Some initial thoughts A couple of days ago one of our customers came up with a question regarding FTS over InnoDB engine. Although the question is not answered in the current article, I came up with the conclusion that FTS is sometimes misunderstood.</description></item><item><title>Multi source data injection to Postgres RDS with encryption and FTS support</title><link>/blog/2016-03/multi-source-data-injection-to-postgres-rds-with-encryption-and-fts-support/</link><pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate><guid>/blog/2016-03/multi-source-data-injection-to-postgres-rds-with-encryption-and-fts-support/</guid><description>Sponsored: Pythian Inc.
Note 1: All the of this presentation is published in this repository. You will find a lot of folders and information, probably part of a blog series.
Note 2: All the work on this article is a POC (Proof of concept).
Note 3: This is something that is related for HIPAA compliant.
KMS/RDS The POC on this article was developed before the releasing of the Key Management service for RDS.</description></item><item><title>Alambre, ugly scripts to save time of your day</title><link>/blog/2016-01/alambre-project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/blog/2016-01/alambre-project/</guid><description>As a DBA we usually have to do dirty things under a running clock. Using that moto, I started a repository to start collecting those nasty things that save time a day. That&amp;rsquo;s what alambre does: small and complex things.
What has inside? bash (preferred) python SQL If you want to indicate issues, go for it at report issues.</description></item></channel></rss>