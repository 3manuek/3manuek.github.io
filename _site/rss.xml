<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>emanuelHub</title>
        <description>emanuelHub - Emanuel Calvo</description>
        <link>https://github.com/3manuek/3manuek.github.io.git</link>
        <atom:link href="https://github.com/3manuek/3manuek.github.io.git/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Sun, 19 Feb 2017 21:30:19 -0300</lastBuildDate>
        <pubDate>Sun, 19 Feb 2017 21:30:19 -0300</pubDate>
        <ttl>60</ttl>


        <item>
                <title>Highlighting Postgres 10 new features: Logical Replication and Partitioning.</title>
                <description>&lt;p&gt;Heya! I this article we are going to explore two of the major features commited in 
the upcoming PostgreSQL release: Logical Replication and Partitioning. Needeless to 
say that these features aren’t yet available in the stable release, so they are prune
to change or extended.&lt;/p&gt;

&lt;h2 id=&quot;logical-replication&quot;&gt;Logical Replication&lt;/h2&gt;

&lt;p&gt;The current logical replication mechanism is a row based decoding, which defers on
those techniques based on &lt;em&gt;statement&lt;/em&gt; in which no matter how many rows are involved
on the source query, they will be shipped as individual rows into the slaves.&lt;/p&gt;

&lt;p&gt;This is something you may want to have in consideration when doing bulk loads, as there
are other tools which can be a better fit than streaming everything from the master.&lt;/p&gt;

&lt;p&gt;Generally speaking, it consist in three &lt;em&gt;visible&lt;/em&gt; elements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a Publication  (source)&lt;/li&gt;
  &lt;li&gt;a Subscription (consumer)&lt;/li&gt;
  &lt;li&gt;and a Logical Replication Slot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The most important and yet probably the more complex is the Logical Replication Slot. 
The magic is done through the &lt;code class=&quot;highlighter-rouge&quot;&gt;pgoutput&lt;/code&gt; plugin, which is the piece of code in charge
of translate the WAL records (&lt;code class=&quot;highlighter-rouge&quot;&gt;pg_wal&lt;/code&gt;) into  entries in the &lt;em&gt;logical log&lt;/em&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;pg_logical&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Is simple: Consumers subscribe to a Publisher using a slot, which contains the snapshot of the
database (the given &lt;em&gt;point in time&lt;/em&gt; of the cluster).&lt;/p&gt;

&lt;p&gt;The full feature is not entirely commited and is expected to count with a &lt;code class=&quot;highlighter-rouge&quot;&gt;WITH COPY DATA&lt;/code&gt;
option at subscription event creation in order to synchronize data from source. Currently,
the patch has some bugs and is in process of review ^&lt;a href=&quot;https://www.postgresql.org/message-id/56f3ec6f1989c738a0fa865b13d25761@xs4all.nl&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although the whole topic is interesting, everything related to Logical Decoding will be ommited
on this article.&lt;/p&gt;

&lt;h2 id=&quot;partitioning&quot;&gt;Partitioning&lt;/h2&gt;

&lt;p&gt;In the past versions, it was possible to reach a very flexible partitioning approach by combining
inheritance and multi-language based triggers. The current implementation does not allow to mix 
inheritance and partitioning but still has some flexibility for detaching and attaching partitions.&lt;/p&gt;

&lt;p&gt;In the current example, we are going to create three partitions with no data, just for keep focus
only on the &lt;em&gt;POC&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;poc&quot;&gt;POC&lt;/h2&gt;

&lt;p&gt;The concept works around on having slaves with a different retention policy of each partitioning by
replicating each on different destinations. As an addition, we are able to create a dummy structure,
to point to each external partitioning for reporting or querying historic data.&lt;/p&gt;

&lt;p&gt;The concept has three types of nodes/databases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A proxy (holding only Foreign Data Wrappers pointing to child tables in inheritance of a dummy table)&lt;/li&gt;
  &lt;li&gt;A master (Containing all the partitions)&lt;/li&gt;
  &lt;li&gt;Shard databases (Only holding the corresponding shard information)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;partitioning-on-the-master-database&quot;&gt;Partitioning on the master database&lt;/h3&gt;

&lt;p&gt;The master database will hold the definitions and the most recent data. The current concept, feeds 
from a Apache Kafka broker’s topic which is partitioned in three. We are going to feed this table
with streams using COPY command.&lt;/p&gt;

&lt;p&gt;The current master DDL is:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_id&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;without&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;zone&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OF&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'P0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OF&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'P1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PARTITION&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;OF&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;VALUES&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'P2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix_main_shard_p0_key&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix_main_shard_p1_key&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ix_main_shard_p2_key&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PUBLICATION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P_main_P0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NOPUBLISH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DELETE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PUBLICATION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P_main_P1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NOPUBLISH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DELETE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PUBLICATION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P_main_P2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FOR&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard2&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NOPUBLISH&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DELETE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;By the current patch, Logical Replication does not support filtering by column content as &lt;code class=&quot;highlighter-rouge&quot;&gt;pglogical&lt;/code&gt; tool.
Even tho is possible to filter by event statement, which still quite useful for our purpose.&lt;/p&gt;

&lt;h3 id=&quot;creating-the-nodes&quot;&gt;Creating the nodes&lt;/h3&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_shard0&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_id&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;without&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;zone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;payload&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now create the SUBSCRIPTION to feed from the corresponding PUBLICATION on the master database.
As the current implementation of the SUBSCRIPTION event does not support with copy data and the
partitions are empty, we are going to create a logical replication slot on the source. This is 
easily done by using the &lt;code class=&quot;highlighter-rouge&quot;&gt;CREATE SLOT&lt;/code&gt; clause. This means that it will set the LSN position from
which the changes must be applied to the destination:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SUBSCRIPTION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P_main_P0&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;CONNECTION&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'port=7777 user=postgres dbname=master'&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;PUBLICATION&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P_main_P0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLOT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It is remarkable to note, that each subscription will create &lt;em&gt;workers&lt;/em&gt; in charge of sending and receiving
those changes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As it is not the scope of this article, I’m going to skip the explanation of the &lt;em&gt;[logical|streamin] replication slots&lt;/em&gt;
in order to keep this readable. Although, it is a core concept of the replication feature.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;bonus-kafka-broker-feeding-example&quot;&gt;Bonus: Kafka broker feeding example&lt;/h4&gt;

&lt;p&gt;Producing fake data to the Kafka broker, composed by &lt;code class=&quot;highlighter-rouge&quot;&gt;key&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;payload&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;randtext&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;cat /dev/urandom | tr -dc &lt;span class=&quot;s1&quot;&gt;'a-zA-Z0-9'&lt;/span&gt; | fold -w 32 | head -n 1&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ; 
  &lt;span class=&quot;k&quot;&gt;do
    for &lt;/span&gt;i &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;seq 1 50&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;  
      &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;uuidgen&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;randtext&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; 
     &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;  | kafkacat -P -b localhost:9092 -qe -K &lt;span class=&quot;s1&quot;&gt;';'&lt;/span&gt; -t PGSHARD 
     sleep 10
  &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Consuming the topic partitionins from the &lt;code class=&quot;highlighter-rouge&quot;&gt;beginning&lt;/code&gt; and setting a limit of &lt;code class=&quot;highlighter-rouge&quot;&gt;100&lt;/code&gt; documents:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/psql -p7777 -Upostgres master &lt;span class=&quot;sh&quot;&gt;&amp;lt;&amp;lt;EOF
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 0 | awk ''{print &quot;P0\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 1 | awk ''{print &quot;P1\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o beginning  -p 2 | awk ''{print &quot;P2\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And then using &lt;code class=&quot;highlighter-rouge&quot;&gt;stored&lt;/code&gt;, in order to consume from the last offset left by the consumer on the group:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/psql -p7777 -Upostgres master &lt;span class=&quot;sh&quot;&gt;&amp;lt;&amp;lt;EOF
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 0 | awk ''{print &quot;P0\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 1 | awk ''{print &quot;P1\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD  -X group.id=1  -o stored  -p 2 | awk ''{print &quot;P2\t\&quot;&quot;$0&quot;\&quot;&quot;}'' ';
EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;querying-from-an-external-database&quot;&gt;Querying from an external database&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE EXTENSION postgres_fdw;
CREATE SERVER shard0 FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS(host '127.0.0.1',port '7777',dbname 'shard0');
CREATE SERVER shard1 FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS(host '127.0.0.1',port '8888',dbname 'shard1');
CREATE SERVER shard2 FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS(host '127.0.0.1',port '9999',dbname 'shard2');

CREATE USER MAPPING FOR postgres SERVER shard0 OPTIONS(user 'postgres');
CREATE USER MAPPING FOR postgres SERVER shard1 OPTIONS(user 'postgres');
CREATE USER MAPPING FOR postgres SERVER shard2 OPTIONS(user 'postgres');

CREATE TABLE main (group_id char(2), payload jsonb);
CREATE FOREIGN TABLE main_shard0 (CHECK (group_id = 'P0'))INHERITS (main) SERVER shard0;
CREATE FOREIGN TABLE main_shard1 (CHECK (group_id = 'P1'))INHERITS (main) SERVER shard1;
CREATE FOREIGN TABLE main_shard2 (CHECK (group_id = 'P2'))INHERITS (main) SERVER shard2;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/postgres10logrepypart</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/postgres-10-logrep-y-part</guid>
                <pubDate>Sat, 18 Feb 2017 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>[Data on Weed] Poor's man sharding technique.</title>
                <description>&lt;h2 id=&quot;if-you-are-sober-stop&quot;&gt;If you are sober, STOP&lt;/h2&gt;

&lt;p&gt;Data on Weed is a series of posts about data technologies with a twist. The twist is that you would never implement this in production
if you are sober or, if you really &lt;em&gt;fuching&lt;/em&gt; know what you are doing.&lt;/p&gt;

&lt;h2 id=&quot;concept&quot;&gt;Concept&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;Poor’s man sharding technique&lt;/em&gt; is nothing more than a combination of two existing features (inheritance and foreign data wrappers)
and one extension (&lt;code class=&quot;highlighter-rouge&quot;&gt;postgres_fdw&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;It consists in a &lt;code class=&quot;highlighter-rouge&quot;&gt;proxy&lt;/code&gt; database and &lt;code class=&quot;highlighter-rouge&quot;&gt;shard&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; databases. Gues what. Yeah, &lt;code class=&quot;highlighter-rouge&quot;&gt;proxy&lt;/code&gt; database holds the entry points and the &lt;code class=&quot;highlighter-rouge&quot;&gt;shard&lt;/code&gt;
databases the data itself.&lt;/p&gt;

&lt;h2 id=&quot;warning&quot;&gt;Warning&lt;/h2&gt;

&lt;p&gt;This is a POC, which I will suggest you think very well if you decide go into this way. It is mostly for didactic purposes or if you
want to drain your brain out of your ears. Still preferable this way other than space bugs.&lt;/p&gt;

&lt;h2 id=&quot;how-this-works&quot;&gt;How this “works”&lt;/h2&gt;

&lt;p&gt;You can have more than one &lt;code class=&quot;highlighter-rouge&quot;&gt;proxy&lt;/code&gt; database, as it does not persist the data. The &lt;code class=&quot;highlighter-rouge&quot;&gt;proxy&lt;/code&gt; database
can be placed in a stand alone instance or on each server. Or anywhere. It does not matter. This
is good as your entry points are not SPOF.&lt;/p&gt;

&lt;p&gt;The concept is very simple, proxy database holds the server definitions and user mappings, along 
with the entry point (table main,without data) and the inherited foreign data wrappers (which do not store data).
This leaves the proxy database with &lt;em&gt;metadata&lt;/em&gt; only. Most importantly, it stores the partition 
function and the trigger defition.&lt;/p&gt;

&lt;p&gt;When an INSERT falls in, trigger executes the function and split the write to the inherited tables
according to the key. This is a fake, as the table is a foreign table which points to a remote database.&lt;/p&gt;

&lt;p&gt;The real tables will be on each &lt;code class=&quot;highlighter-rouge&quot;&gt;shardXX&lt;/code&gt; database, the location is up to the engineer.&lt;/p&gt;

&lt;h2 id=&quot;no-magic-at-all&quot;&gt;No magic at all&lt;/h2&gt;

&lt;p&gt;As you expect, life sucks and sometimes one does not have time to dig into every crazy idea that prompts.
Special mention over those that come under doubtely states of consciousness.&lt;/p&gt;

&lt;p&gt;For HA, SERVERs can point to slaves, but beware of this as you must need to change the
FOREIGN TABLE definition and promote them, otherwise &lt;em&gt;writes won’t work and reads will be stale&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This technique is very simplistic, and it lacks of a lot of features provided for sharding tools.&lt;/p&gt;

&lt;p&gt;Postgres 9.6 has a nice set of improvements on FDWs, specially related with condition pushdowns.
Being said, this kind of things will definitively a PITA under previous versions.&lt;/p&gt;

&lt;h2 id=&quot;proxy-database&quot;&gt;proxy database&lt;/h2&gt;

&lt;p&gt;I’m going to create two shards, because it’s hard to code being stoned. Actually I’m just lazy,
but my friend Elrich Bachman told me that if it doesn’t come in pairs, one is artificial.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE EXTENSION postgres_fdw;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;These definitions are thw ones you’ll need to extend your shard:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE SERVER shard1_main FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS(host '127.0.0.1',port '5434',dbname 'shard1');
CREATE SERVER shard2_main FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS(host '127.0.0.1',port '5435',dbname 'shard2');

CREATE USER MAPPING FOR postgres SERVER shard1_main OPTIONS(user 'postgres');
CREATE USER MAPPING FOR postgres SERVER shard2_main OPTIONS(user 'postgres');
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Create the tables, function and trigger. Obviously, you will need to create foreign tables as shards
you are planning to have:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE main (shardKey char(2), key bigint, avalue text);
CREATE FOREIGN TABLE main_shard01 (CHECK (shardKey = '01'))INHERITS (main) SERVER shard1_main;
CREATE FOREIGN TABLE main_shard02 (CHECK (shardKey = '02'))INHERITS (main) SERVER shard2_main;

CREATE OR REPLACE FUNCTION f_main_part() RETURNS TRIGGER AS
$FMAINPART$
DECLARE
            partition_name text;
BEGIN
            partition_name := 'main_shard' || NEW.shardKey;
            EXECUTE  'INSERT INTO ' ||  quote_ident(partition_name) ||  ' SELECT ($1).*' USING NEW ;
            RETURN NULL;
END;
$FMAINPART$ LANGUAGE plpgsql;

CREATE TRIGGER t_main BEFORE INSERT ON main FOR EACH ROW EXECUTE PROCEDURE f_main_part();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;shards&quot;&gt;shards&lt;/h2&gt;

&lt;p&gt;On shards is easier:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE main_shard01(shardKey char(2), key bigint, avalue text, CHECK(shardKey='01'));
CREATE INDEX ON main_shard01(key);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;test&quot;&gt;TEST&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INSERT INTO main VALUES ('01',1,'trololol'),('01',2,random()::text),('02',2,random()::text);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;grab-them-from-the-hidden-columns&quot;&gt;&lt;em&gt;Grab them from the hidden columns&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;The tableoid is the shard one, not the proxy foreign definition, this is important.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;proxy=# select tableoid,* from main;
 tableoid | shardkey | key |      avalue       
----------+----------+-----+-------------------
    16422 | 01       |   1 | trololol
    16422 | 01       |   2 | 0.544912547804415
    16426 | 02       |   2 | 0.459446560591459
(3 rows)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/poorsmansharding</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/Poors-man-sharding-technique</guid>
                <pubDate>Tue, 31 Jan 2017 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Shard Buffer Pool With Proxysql</title>
                <description>
&lt;p&gt;docker-compose up node01
docker-compose scale nodeN=2
docker-compose scale proxysql=2
docker pull quay.io/coreos/etcd
docker run -d –name discovery -p 2379:2379 -p 2380:2380 -v /usr/share/ca-certificates/:/etc/ssl/certs –net host quay.io/coreos/etcd:v2.0.9 -name discovery -initial-advertise-peer-urls http://${IP}:2380,http://${IP}:7001 -listen-peer-urls http://${IP}:2380,http://${IP}:7001 -initial-cluster discovery=http://${IP}:2380,discovery=http://${IP}:7001 -advertise-client-urls http://${IP}:2379,http://${IP}:4001 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001&lt;/p&gt;

&lt;p&gt;docker run -d –name discovery -p 2379:2379 -p 2380:2380 –net host quay.io/coreos/etcd -name discovery -initial-advertise-peer-urls http://${IP}:2380,http://${IP}:7001 -listen-peer-urls http://${IP}:2380,http://${IP}:7001 -initial-cluster discovery=http://${IP}:2380,discovery=http://${IP}:7001 -advertise-client-urls http://${IP}:2379,http://${IP}:4001 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001&lt;/p&gt;

&lt;p&gt;https://github.com/percona/proxysql-admin-tool
https://coreos.com/etcd/docs/latest/v2/docker_guide.html&lt;/p&gt;

&lt;p&gt;ETCD_HOST=${ETCD_HOST:-10.20.2.4:2379}
docker run -d -v /usr/share/ca-certificates/:/etc/ssl/certs -p 4001:4001 -p 2380:2380 -p 2379:2379 \
 –name etcd quay.io/coreos/etcd \
 -name etcd0 \
 -advertise-client-urls http://${ETCD_HOST}:2379,http://${ETCD_HOST}:4001 \
 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \
 -initial-advertise-peer-urls http://${ETCD_HOST}:2380 \
 -listen-peer-urls http://0.0.0.0:2380 \
 -initial-cluster-token etcd-cluster-1 \
 -initial-cluster etcd0=http://${ETCD_HOST}:2380 \
 -initial-cluster-state new&lt;/p&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/2016/11/26/shard-buffer-pool-with-proxysql</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/2016/11/26/shard-buffer-pool-with-proxysql</guid>
                <pubDate>Sat, 26 Nov 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Mysql And Oom</title>
                <description>
&lt;p&gt;OOM killer is a OS mechanism to avoid processes to consume more resources than the available in the host.
It may look as a good idea for avoid scenarios wether certain proceses behave unexpectely.&lt;/p&gt;

&lt;p&gt;swap configuration vm.swappiness=1 
overcommit memory, applies to MySQL ?&lt;/p&gt;

&lt;p&gt;There is another mechanism wether the OS &lt;em&gt;scores&lt;/em&gt; processes. THe higher the score, the more likely to be killed
the process will be. I’ve been seeing fixes around by setting -20 or other numbers to the oom_score_adj. You may
be aware that this only applies an arithmetic operations to the current score, but it won’t be a permanent fix,
as scores changes according to the process status. The best source is &lt;a href=&quot;http://man7.org/linux/man-pages/man5/proc.5.html&quot;&gt;the proc man page&lt;/a&gt;.
However, oom_adj is deprecated in 2.6.36 in favor of oom_score_adj.&lt;/p&gt;

&lt;p&gt;http://www.oracle.com/technetwork/articles/servers-storage-dev/oom-killer-1911807.html&lt;/p&gt;

&lt;p&gt;No consensus on an elegant way to do this:&lt;/p&gt;

&lt;p&gt;https://groups.google.com/a/percona.com/forum/?hl=en#!msg/experts/78USb3jqJnA/3AlUYsLXEgAJ;context-place=forum/experts&lt;/p&gt;

&lt;p&gt;Nor neiether Maria:
https://jira.mariadb.org/browse/MDEV-9264&lt;/p&gt;

&lt;p&gt;persist in cron line:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;*/1 * * * * root (pidof mysqld&lt;/td&gt;
      &lt;td&gt;while read PID; do echo -1000 &amp;gt; /proc/$PID/oom_score_adj; done)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The -1000 has a reason, and it is explained clearly in the manpage:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The value of oom_score_adj is added to the badness score
             before it is used to determine which task to kill.  Acceptable
             values range from -1000 (OOM_SCORE_ADJ_MIN) to +1000
             (OOM_SCORE_ADJ_MAX).  This allows user space to control the
             preference for OOM-killing, ranging from always preferring a
             certain task or completely disabling it from OOM killing.  The
             lowest possible value, -1000, is equivalent to disabling OOM-
             killing entirely for that task, since it will always report a
             badness score of 0.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As oom_score is a setting that can increase thourgh server acitivty, you may want to ensure that is
always 0.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# cat /proc/$(pidof mysqld)/oom_score
0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;What about overcommit:
https://www.kernel.org/doc/Documentation/vm/overcommit-accounting&lt;/p&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/2016/10/10/MySQL-and-OOM</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/2016/10/10/MySQL-and-OOM</guid>
                <pubDate>Mon, 10 Oct 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Go-Plus and Atom GOPATH fix</title>
                <description>&lt;h2 id=&quot;the-background&quot;&gt;The background&lt;/h2&gt;

&lt;p&gt;Golang is an awesome language, but I found it pretty unstable within the environment variables (at least in macOS Sierra/El Capitan). &lt;code class=&quot;highlighter-rouge&quot;&gt;gvm&lt;/code&gt; is your friend btw, and it helped me to fix some of the issues by installing the latest release candidate of the 1.7.1 series.&lt;/p&gt;

&lt;p&gt;Keep in mind that if you want to upgrade your macOS to Sierra, you’ll  need to backup all of your environment variables and reinstall &lt;code class=&quot;highlighter-rouge&quot;&gt;gvm&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Atom has a plugin for Golang, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;go-plus&lt;/code&gt;, and if you are reading this is because the documentation around isn’t very helpful.&lt;/p&gt;

&lt;h2 id=&quot;the-problem&quot;&gt;The problem&lt;/h2&gt;

&lt;p&gt;GOPATH is not been loaded! Also you may see several errors when Atom is trying to get packages using &lt;code class=&quot;highlighter-rouge&quot;&gt;go-get&lt;/code&gt;. Nor neither GOBIN environment variable.&lt;/p&gt;

&lt;p&gt;Also, I’ve been having issues with the following (gocode is one of the packages for the Atom’s plugin, but it does happen with any package):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go install github.com/nsf/gocode: open /bin/gocode: operation not permitted
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;

&lt;p&gt;The solution for the GOPATH is simple. There is a &lt;em&gt;warning&lt;/em&gt; when this happens but it’s been added recently, with the HINT to start from the command line for fixing this.&lt;/p&gt;

&lt;p&gt;That’s easy. An &lt;code class=&quot;highlighter-rouge&quot;&gt;atom &amp;amp;&lt;/code&gt; from terminal should fix this by loading the environment variables. However, keep in mind that GOBIN needs to be on the path! You may need to create a bin folder in your &lt;em&gt;go workspace&lt;/em&gt;. Also, don’t forget to add those variables into your shell &lt;em&gt;.*rc&lt;/em&gt; file (.bashrc, .zshrc, .profile).&lt;/p&gt;

&lt;p&gt;i.e.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p ~/go/bin
export GOPATH=$HOME/go
export GOBIN=$HOME/go/bin
nohup atom &amp;amp;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hope it fixes your day!&lt;/p&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/goplusatomcopath</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/GoPlus-and-Atom-GOPATH-fix-HOWTO</guid>
                <pubDate>Wed, 05 Oct 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>PostgreSQL RDS pg-stat-ramdisk-size new feature and its calculations</title>
                <description>&lt;h2 id=&quot;what-does-it-change-and-why-is-so-important&quot;&gt;What does it change and why is so important?&lt;/h2&gt;

&lt;p&gt;Tracking databases and &lt;em&gt;not just tables&lt;/em&gt; counters in Postgres isn’t cheap, but since some time ago there were workarounds involving the setup of a ramdisk to place the directory pointed by &lt;code class=&quot;highlighter-rouge&quot;&gt;stat_temp_directory&lt;/code&gt; GUC variable. That directory places a &lt;code class=&quot;highlighter-rouge&quot;&gt;global.stat&lt;/code&gt; and a per-database stat files called like &lt;code class=&quot;highlighter-rouge&quot;&gt;db_&amp;lt;oidOfDB&amp;gt;.stat&lt;/code&gt;. Although the mechanism for writing into these files avoids extra or unnecessary flushes, it is very write intensive.&lt;/p&gt;

&lt;p&gt;This change does not require any downtime (in standalone installations), as a simple reload will force the Stat Collector to rewrite the files on the folder. There is a pretty much clear blog on &lt;a href=&quot;http://hacksoclock.blogspot.com.ar/2014/04/putting-statstempdirectory-on-ramdisk.html&quot;&gt;putting stat_temp_directory on a ramdisk&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The problem relies on the RDS lack of privileges to manipulate file or directory contents, which does not allow you to check the current size and set a proper value. Although, you may probably want to know that there is a limit of &lt;em&gt;1 GB&lt;/em&gt; for this setting in RDS.&lt;/p&gt;

&lt;p&gt;If you don’t want any further details and you want to relief your storage, set it to 256 MB and continue with your life. Even though is a large setting (next paragraph explain why), you don’t want to fall short on it.&lt;/p&gt;

&lt;p&gt;After you apply the change over &lt;code class=&quot;highlighter-rouge&quot;&gt;pg_stat_ramdisk_size&lt;/code&gt;, you will see the location in the RDS have changed:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;show stats_temp_directory;
   stats_temp_directory    
---------------------------
 /rdsdbramdisk/pg_stat_tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;tldr-whats-the-expected-size-of-the-stat_temp_directory&quot;&gt;TL;DR &lt;em&gt;What’s the expected size of the stat_temp_directory&lt;/em&gt;?&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Structure/Constant&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PGSTAT_FILE_FORMAT_ID&lt;/td&gt;
      &lt;td&gt;1 byte&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_StatTabEntry&lt;/td&gt;
      &lt;td&gt;164 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_StatFuncEntry&lt;/td&gt;
      &lt;td&gt;28 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;closingChar&lt;/td&gt;
      &lt;td&gt;‘E’&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;describers&lt;/td&gt;
      &lt;td&gt;char (T or F in this case)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;First of all, as it’ll explained later, not all the tables, indexes and functions are written on the &lt;em&gt;db statsfile&lt;/em&gt;. Basically, a basic formula will be &lt;em&gt;SizeOfDBStatFile = PGSTAT_FILE_FORMAT_ID + describers + (tableCount * PgStat_StatTabEntry) + (funcCount * PgStat_StatFuncEntry) + closingChar&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Query 1) will give you the estimate for the tables &lt;em&gt;if all of them were flushed on the file&lt;/em&gt;. Also, you need to do the same within &lt;code class=&quot;highlighter-rouge&quot;&gt;pg_proc&lt;/code&gt;, but instead the factor will be 28 bytes. You’ll need to run this on every database, and sum them all.&lt;/p&gt;

&lt;p&gt;Query 1)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select count(*) * 164 &quot;size in bytes&quot; from pg_class where relkind ('r','i','S')
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This database statfile is one &lt;em&gt;per database&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;global-stats&quot;&gt;Global Stats&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Structure&lt;/th&gt;
      &lt;th&gt;Size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_StatDBEntry&lt;/td&gt;
      &lt;td&gt;180 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_GlobalStats&lt;/td&gt;
      &lt;td&gt;92 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_ArchiverStats&lt;/td&gt;
      &lt;td&gt;114 bytes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;describer&lt;/td&gt;
      &lt;td&gt;char (‘D’)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The global statfile is smaller, and contains only the global stats and the counters across databases. Should be something close to &lt;em&gt;PGSTAT_FILE_FORMAT_ID +describer + PgStat_GlobalStats + PgStat_ArchiverStats + (PgStat_StatDBEntry + describer) * numDatabases&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So, as you can see, the limitation imposed by AWS in regarding is way above the amount of data held on this directory in most of the databases that can run inside RDS expectations.&lt;/p&gt;

&lt;h2 id=&quot;why-it-affects-rds&quot;&gt;Why it affects RDS?&lt;/h2&gt;

&lt;p&gt;Prior to this feature been added, the &lt;code class=&quot;highlighter-rouge&quot;&gt;stat_temp_directory&lt;/code&gt; had a place into the persistent storage layer. This was the same as any other Postgres installation by default, however due to the storage characteristics of RDS the impact could be considered higher than a standalone setup.&lt;/p&gt;

&lt;p&gt;If your application is write intensive, you will see the impact on the Write latency and operations.&lt;/p&gt;

&lt;h2 id=&quot;a-deeper-look&quot;&gt;A deeper look&lt;/h2&gt;

&lt;p&gt;So the &lt;a href=&quot;http://dba.stackexchange.com/questions/150474/how-to-determine-optimal-value-for-pg-stat-ramdisk-size-on-amazon-rds/150579#150579&quot;&gt;question&lt;/a&gt; didn’t took much time to appear in the network and, I wasn’t the exception. Is there a way to pre calculate the contents of the directory?&lt;/p&gt;

&lt;p&gt;I couldn’t end up with an exact number however, you may know that the size of the files are more related to the number of tables, indexes, functions and databases. The following structure is the core of this implementation. It is so important that it actually has a defined &lt;code class=&quot;highlighter-rouge&quot;&gt;PGSTAT_FILE_FORMAT_ID&lt;/code&gt; that it is written also in the stat files.&lt;/p&gt;

&lt;p&gt;All the structures for these file contents are placed in the &lt;code class=&quot;highlighter-rouge&quot;&gt;include/pgstat.h&lt;/code&gt; header and its implementation is done in &lt;code class=&quot;highlighter-rouge&quot;&gt;postmaster/pgstat.c&lt;/code&gt; (as it is a startup worker). Every field that is used for counters use &lt;code class=&quot;highlighter-rouge&quot;&gt;int64&lt;/code&gt; and there are some &lt;code class=&quot;highlighter-rouge&quot;&gt;timestampz&lt;/code&gt; (64 bits too) with Oid as an exception, which is represented by 32 bits (unsigned int).&lt;/p&gt;

&lt;p&gt;Backends communicate to the collector through &lt;code class=&quot;highlighter-rouge&quot;&gt;StatMsgType&lt;/code&gt; struct, when is different from a zeroed struct &lt;code class=&quot;highlighter-rouge&quot;&gt;PgStat_TableCounts&lt;/code&gt;. Structures kept in backend local memory while accumulating counts. So, that means that not all the tables, indexes and functions will have an entry.&lt;/p&gt;

&lt;p&gt;Which backends can request a file write? All the backends, the archiver, the bgwriter. All of them use the same structure for passing the changes (PgStat_Msg).&lt;/p&gt;

&lt;p&gt;There are 2 functions for write (pgstat_write_db_statsfile, pgstat_write_statsfiles) and 2 for read (pgstat_read_db_statsfile,pgstat_read_statsfiles) each of those controlling either the &lt;code class=&quot;highlighter-rouge&quot;&gt;db_&amp;lt;oid&amp;gt;.stat&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;global.stat&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;h3 id=&quot;pgstat_statdbentry&quot;&gt;PgStat_StatDBEntry&lt;/h3&gt;

&lt;p&gt;The HTAB structure is opaque, and it holds a hash map of tables and functions to be collected. We don’t care about the size of this maps as it won’t be written to the stats file anyway. The whole database entry is 22 * 64 bit values + 1 * 32 bits, per database (180 bytes).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#define PGSTAT_FILE_FORMAT_ID   0x01A5BC9D
typedef struct PgStat_StatDBEntry
{
        /*
        NOTE:
        The oid type is currently implemented as an unsigned four-byte integer.
            typedef unsigned int Oid;
        */
        Oid                     databaseid;
        PgStat_Counter n_xact_commit;
        PgStat_Counter n_xact_rollback;
        PgStat_Counter n_blocks_fetched;
        PgStat_Counter n_blocks_hit;
        PgStat_Counter n_tuples_returned;
        PgStat_Counter n_tuples_fetched;
        PgStat_Counter n_tuples_inserted;
        PgStat_Counter n_tuples_updated;
        PgStat_Counter n_tuples_deleted;
        TimestampTz last_autovac_time;
        PgStat_Counter n_conflict_tablespace;
        PgStat_Counter n_conflict_lock;
        PgStat_Counter n_conflict_snapshot;
        PgStat_Counter n_conflict_bufferpin;
        PgStat_Counter n_conflict_startup_deadlock;
        PgStat_Counter n_temp_files;
        PgStat_Counter n_temp_bytes;
        PgStat_Counter n_deadlocks;
        PgStat_Counter n_block_read_time;       /* times in microseconds */
        PgStat_Counter n_block_write_time;

        TimestampTz stat_reset_timestamp;
        TimestampTz stats_timestamp;    /* time of db stats file update */

        /*
         * tables and functions must be last in the struct, because we don't write
         * the pointers out to the stats file.
         */
        HTAB       *tables;             // defined in utils/hsearch.h
        HTAB       *functions;
} PgStat_StatDBEntry;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;structures&quot;&gt;Structures&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Structure&lt;/th&gt;
      &lt;th&gt;Detail&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_StatTabEntry&lt;/td&gt;
      &lt;td&gt;20 * 64 bits and 1 * 32 Oid&lt;/td&gt;
      &lt;td&gt;(164 bytes)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_StatFuncEntry&lt;/td&gt;
      &lt;td&gt;3 * 64 bits and 1 * 32 Oid&lt;/td&gt;
      &lt;td&gt;(28 bytes)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_GlobalStats&lt;/td&gt;
      &lt;td&gt;11 * 64 bits, 8 bytes + 1 * 32 bit, 4 bytes&lt;/td&gt;
      &lt;td&gt;(92 bytes)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PgStat_ArchiverStats&lt;/td&gt;
      &lt;td&gt;4 *  8bytes, 2 char 41 bytes.&lt;/td&gt;
      &lt;td&gt;(114 bytes)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/pgstatramdisksize</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/RDS-pg-stat-ramdisk-size-on-RDS</guid>
                <pubDate>Sun, 25 Sep 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Setting up PostgreSQL with Docker official image.</title>
                <description>&lt;h2 id=&quot;why-postgres-in-docker&quot;&gt;Why Postgres in Docker?&lt;/h2&gt;

&lt;h2 id=&quot;resources-and-links&quot;&gt;Resources and Links&lt;/h2&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/postgresdocker</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/postgres-docker</guid>
                <pubDate>Thu, 01 Sep 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Verificando descargas a traves de hashes y firmas</title>
                <description>&lt;h2 id=&quot;la-razón&quot;&gt;La razón&lt;/h2&gt;

&lt;p&gt;Bitcoin es en mi opinión, la más reciente y revolucionaria tecnología, que permite
monetizar la capacidad de computo. No voy a entrar en detalles con respecto a
como funciona o como se utiliza, ya que estimo que si llegaste a esta página, es
porque te has topado con &lt;a href=&quot;https://bitcoin.org/en/alert/2016-08-17-binary-safety&quot;&gt;algo como esto&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;El &lt;em&gt;hasheo&lt;/em&gt; de binarios y las firmas digitales han estado dando vuelta desde hace
años. Sin embargo, es sorprendente la cantidad de personas que aún no lo toman en serio.&lt;/p&gt;

&lt;p&gt;La única herramienta que tenemos como ciudadanos contra la vigilancia de estado
y el ciberterrorismo, es nuestro propio conocimiento. Ya que los estados no pueden
garantizar la protección de datos de sus ciudadanos debido a la constante mejora
de los sistemas de espionaje y avances en materia de ciberataques.&lt;/p&gt;

&lt;p&gt;Hay un muy buen tutorial &lt;em&gt;exclusivo para el Bitcoin Core&lt;/em&gt; en &lt;a href=&quot;https://www.reddit.com/r/Bitcoin/wiki/verifying_bitcoin_core&quot;&gt;reddit&lt;/a&gt;,
pero está en inglés.&lt;/p&gt;

&lt;p&gt;Más allá de Bitcoin, tené en cuenta que la firma de binarios es algo más que rutinario y por consecuencia, recomendable verificar antes de poner código
en producción.&lt;/p&gt;

&lt;h2 id=&quot;obteniendo-los-hashes&quot;&gt;Obteniendo los &lt;em&gt;hashes&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Cada proyecto tiene sus formas, pero vamos a seguir las del Bitcoin Core. Al
ir a la página de los &lt;a href=&quot;https://bitcoin.org/en/download&quot;&gt;&lt;em&gt;downloads&lt;/em&gt;&lt;/a&gt;, vamos a ver
un link a &lt;a href=&quot;https://bitcoin.org/bin/bitcoin-core-0.12.1/SHA256SUMS.asc&quot;&gt;Verify release signatures&lt;/a&gt;, que contiene un archivo con un mensaje en texto claro y una firma codificada. La parte codificada, contiene el texto claro codificado con la firma que descargaremos/importaremos más adelante:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Descargo:&lt;/span&gt;
curl https://bitcoin.org/bin/bitcoin-core-0.12.1/SHA256SUMS.asc &amp;gt; SHA256SUMS.asc


&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl https://bitcoin.org/bin/bitcoin-core-0.12.1/SHA256SUMS.asc
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

abf0e7336621250702d7a55487c85b8de33c07a30fbc3ecf7f56c97007fcb4ce  bitcoin-0.12.1-linux32.tar.gz
54aca14b7512801ab78cc93f8576e1b66364a890e8017e8a187e4bf0209fd28c  bitcoin-0.12.1-linux64.tar.gz
91d14dcb9b88ca845df450ceb94250bb5c9a0d514d8ca0c55eb480d0ac77ef32  bitcoin-0.12.1-osx64.tar.gz
e1bc86d24dd978d64b511ada68be31057c20789fb9a6a86c40043a32bf77cb05  bitcoin-0.12.1-osx.dmg
08fc3b6c05c39fb975bba1f6dd49992df46511790ce8dc67398208af9565e199  bitcoin-0.12.1.tar.gz
fba73e4825a6421ce6cc1e48b67ff5f2847ae1b520d26272e69f7f25de4f36d1  bitcoin-0.12.1-win32-setup.exe
148fb438a32f1706a366a7825bbc5e770e5f9a20e5694f724a443275976a0791  bitcoin-0.12.1-win32.zip
c6e06f90e41c36c9a447f065952869e2d7d571ab34b86d061ae19ec25b2799d4  bitcoin-0.12.1-win64-setup.exe
d8e1ab9ff65b79c130ec6af8e36626310ffdaf6aacb7a40cfb76e7a63bdfcfd5  bitcoin-0.12.1-win64.zip
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;GNU/Linux&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

iQIcBAEBCAAGBQJXEIwgAAoJEJDIAZ42wulkXNcP/Re0iawPi8muiq6J36ZUZKws
KL2nwjCImj91on8wGoTUir1IytuIafA4JMHslos2Ak3za2UKAEZrEfx0dXm/FVql
AgRneYLYedMQ8127UkSho4rxuwjB3h2gR/FGPpPT0PmbNTWOFsKtV1V9zwsCeA9Q
br/ly2BfZHWsS1tpSK5ukP5W0q+Ii2fO4pcfaAsS2y/gc5kyj5hTiKQivwBVXoVA
cyH1splq1foM5BYwOuT/cUKGrpA8fWo7+xOaEhhFBlW0oJaSXcNSK9mVTSI/dQ/2
lINXcWBtotnH6/evS35pAIOe4PHg/URhXNT/Sdfwts4YL5nMtF+SPBrJWadPvx3C
qdSDZKMuM0cDjVg1F4rjoWAxyshWNKKU2J+qkNUBZ1LbpVyDR3Gl4LFwRjaw0wyZ
n6zHonPCtp33ErhsaY0GryHV1pKvL1h6uyDNWHbYpKny4F+TvbyQ6XNVHrx1IAn5
+9UMPB3Q962/8hrRqK95Cs6AJ/D1Wdw9rwEqOC48waDzttYCVknn4L6rGECDdRM4
6pbWNTf3m9lzThWjiuEdNnPoNuKoBD9/UHWW/WRHjT6tbcGqstoyRKTsi8jjmwnC
9g4xWRsTdqYIAL4PBv32T+QYW/YcyRNTT97t/M0aukXxxxjCObehWVmBXVeNn0/9
lvvCgGgSJXtJHxzqcJ2I
&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;a2/6
-----END PGP SIGNATURE-----
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Podemos observar que el hash es un algoritmo de SHA256 (justo debajo del &lt;em&gt;BEGIN PGP SIGNED MESSAGE&lt;/em&gt;). En Linux tenemos el comando &lt;code class=&quot;highlighter-rouge&quot;&gt;sha256sum&lt;/code&gt;,
pero en Mac vamos a usar (no hay mucha diferencia en el proceso) &lt;code class=&quot;highlighter-rouge&quot;&gt;shasum&lt;/code&gt;. Lo que vamos a hacer es
hashear los binarios que descargamos y compararlos con la tabla anterior:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ grep $(shasum -a 256 bitcoin-0.12.1-osx.dmg) SHA256SUMS.asc
SHA256SUMS.asc:e1bc86d24dd978d64b511ada68be31057c20789fb9a6a86c40043a32bf77cb05  bitcoin-0.12.1-osx.dmg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hay varios algoritmos de hashing, por lo que dependiendo el proyecto o binarios
que necesites verificar, deberás utilizar uno u otro. Por ejemplo, &lt;code class=&quot;highlighter-rouge&quot;&gt;md5sum&lt;/code&gt; o &lt;code class=&quot;highlighter-rouge&quot;&gt;md5&lt;/code&gt; (Mac) son las
herramientas para otro algoritmo llamado &lt;em&gt;Message-Digest Algorithm 5&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;¡Joya! Ahora vamos a verificar la firma.&lt;/p&gt;

&lt;h2 id=&quot;la-firma&quot;&gt;La &lt;em&gt;firma&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;Para la versión que yo tenía bajada, la firma es la key &lt;code class=&quot;highlighter-rouge&quot;&gt;36C2E964&lt;/code&gt;. En la página
de las descargas, podrás ver las firmas para cada versión. En este caso, la firma
es la de &lt;a href=&quot;https://bitcoin.org/laanwj.asc&quot;&gt;Wladimir&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Importo:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ gpg --import Downloads/laanwj-releases.asc
gpg: key 36C2E964: public key &quot;Wladimir J. van der Laan (Bitcoin Core binary release signing key) &amp;lt;laanwj@gmail.com&amp;gt;&quot; imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model
gpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Verifico el fingerprint (podrás ver el fingerprint en el &lt;a href=&quot;https://bitcoin.org/en/alert/2016-08-17-binary-safety&quot;&gt;anuncio&lt;/a&gt; o cuando decodifiques el mensaje):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ gpg --fingerprint 01EA5486DE18A882D4C2684590C8019E36C2E964
pub   4096R/36C2E964 2015-06-24 [expires: 2017-02-13]
      Key fingerprint = 01EA 5486 DE18 A882 D4C2  6845 90C8 019E 36C2 E964
uid                  Wladimir J. van der Laan (Bitcoin Core binary release signing key) &amp;lt;laanwj@gmail.com&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Con esto sabemos que la firma es válida. Lo que no sabemos es si el mensaje fue
firmado con esta firma. Por suerte esto es más que sencillo, solo necesitamos
ver que podemos decodificar el mensaje utilizando la firma que importamos:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Emanuels-iMac:~ emanuel$ gpg --output mensaje  -d Downloads/SHA256SUMS.asc
gpg: Signature made Fri Apr 15 03:37:20 2016 ART using RSA key ID 36C2E964
gpg: Good signature from &quot;Wladimir J. van der Laan (Bitcoin Core binary release signing key) &amp;lt;laanwj@gmail.com&amp;gt;&quot;
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 01EA 5486 DE18 A882 D4C2  6845 90C8 019E 36C2 E964
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Podemos ver que el fingerprint concuerda con el del anuncio. Eso es una buena señal.&lt;/p&gt;

&lt;p&gt;Contenido del mensaje decodificado éxitosamente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cat mensaje
abf0e7336621250702d7a55487c85b8de33c07a30fbc3ecf7f56c97007fcb4ce  bitcoin-0.12.1-linux32.tar.gz
54aca14b7512801ab78cc93f8576e1b66364a890e8017e8a187e4bf0209fd28c  bitcoin-0.12.1-linux64.tar.gz
91d14dcb9b88ca845df450ceb94250bb5c9a0d514d8ca0c55eb480d0ac77ef32  bitcoin-0.12.1-osx64.tar.gz
e1bc86d24dd978d64b511ada68be31057c20789fb9a6a86c40043a32bf77cb05  bitcoin-0.12.1-osx.dmg
08fc3b6c05c39fb975bba1f6dd49992df46511790ce8dc67398208af9565e199  bitcoin-0.12.1.tar.gz
fba73e4825a6421ce6cc1e48b67ff5f2847ae1b520d26272e69f7f25de4f36d1  bitcoin-0.12.1-win32-setup.exe
148fb438a32f1706a366a7825bbc5e770e5f9a20e5694f724a443275976a0791  bitcoin-0.12.1-win32.zip
c6e06f90e41c36c9a447f065952869e2d7d571ab34b86d061ae19ec25b2799d4  bitcoin-0.12.1-win64-setup.exe
d8e1ab9ff65b79c130ec6af8e36626310ffdaf6aacb7a40cfb76e7a63bdfcfd5  bitcoin-0.12.1-win64.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;para-los-oneliners&quot;&gt;Para los &lt;em&gt;oneLiners&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Siempre hay algun &lt;em&gt;freak&lt;/em&gt; que quiere cosas como estas (está para Mac, usar &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;sha256sum&lt;/code&gt; para Linux):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; curl https://bitcoin.org/bin/bitcoin-core-0.12.1/SHA256SUMS.asc  | gpg -d ; &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 2&amp;gt; /dev/null  | grep &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;shasum -a 256 bitcoin-0.12.1-osx.dmg | cut -f1 -d&lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
e1bc86d24dd978d64b511ada68be31057c20789fb9a6a86c40043a32bf77cb05  bitcoin-0.12.1-osx.dmg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;unas-últimas-sugerencias&quot;&gt;Unas últimas sugerencias&lt;/h2&gt;

&lt;p&gt;‘ta jodida la mano, así que mejor arremangarse y empezar a corroborar toda
esas cosas que descargamos, en especial aquellas que están en la mira.&lt;/p&gt;

&lt;p&gt;Listo el pollo. ¡Espero que te haya servido!&lt;/p&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/pgp-bitcoint-sha</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/verificar-hashes-y-firmas-en-binarios</guid>
                <pubDate>Thu, 18 Aug 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>redshift.</title>
                <description>&lt;p&gt;STV tables are actually virtual system tables that contain snapshots of the current system data.&lt;/p&gt;

&lt;p&gt;http://docs.aws.amazon.com/redshift/latest/dg/c_intro_STV_tables.html&lt;/p&gt;

&lt;p&gt;STL Tables for Logging  http://docs.aws.amazon.com/redshift/latest/dg/c_intro_STL_tables.html&lt;/p&gt;

&lt;p&gt;SV (system views)&lt;/p&gt;

&lt;p&gt;http://docs.aws.amazon.com/redshift/latest/dg/c_intro_system_views.html&lt;/p&gt;

&lt;p&gt;https://aws.amazon.com/redshift/faqs/&lt;/p&gt;

&lt;p&gt;class Meta:
        db_table = ‘request_stats_2015’
        diststyle = base.RedshiftDiststyles.KEY
        distkey = ‘identity_id’
        sortkey = (‘created_at’, ‘store_id’, ‘resource_type’, ‘action’)
        compression = dict(
            id=base.RedshiftCompressionTypes.BYTEDICT,
            created_at=base.RedshiftCompressionTypes.DELTA32K,
            store_id=base.RedshiftCompressionTypes.BYTEDICT,
            identity_id=base.RedshiftCompressionTypes.BYTEDICT,
            remote_address=base.RedshiftCompressionTypes.BYTEDICT,
            resource_type=base.RedshiftCompressionTypes.BYTEDICT,
            resource_id=base.RedshiftCompressionTypes.TEXT255,
            action=base.RedshiftCompressionTypes.TEXT255,
            os=base.RedshiftCompressionTypes.BYTEDICT,
            client=base.RedshiftCompressionTypes.BYTEDICT,
            data=base.RedshiftCompressionTypes.LZO
        )&lt;/p&gt;

&lt;p&gt;https://github.com/iMedicare/shared/blob/master/imedicare/shared/historical/request_stat.py#L29-L45&lt;/p&gt;

&lt;p&gt;Distribution of reads across disks:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;historical=# select host , diskno, sum(reads),sum(writes),sum(mbps),sum(seek_forward), sum(seek_back), max(used) from stv_partitions group by 1,2;
 host | diskno |  sum  |   sum   | sum | sum  | sum  |  max  
------+--------+-------+---------+-----+------+------+-------
    1 |      0 |  8408 | 1667511 |   0 | 2915 | 2273 | 90125
    2 |      0 | 20194 | 1523545 |   0 | 9389 | 4061 | 96542
    0 |      0 |  8010 | 1611316 |   0 | 2838 | 2235 | 96542
(3 rows)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;http://docs.aws.amazon.com/redshift/latest/dg/r_STV_PARTITIONS.html&lt;/p&gt;

&lt;p&gt;explain  SELECT max(created_at) as created_at
from request_stats_2015
WHERE (((“resource_type” = ‘patients’) AND
           (“store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’)) 
AND  (“remote_address” &amp;lt;&amp;gt; ‘10.0/16’))&lt;/p&gt;

&lt;p&gt;explain SELECT max(t1.created_at), t1.client
FROM 
( SELECT created_at, client
request_stats_2015
WHERE (((“resource_type” = ‘tokens’) AND
           (“store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’)) AND
          NOT (“remote_address” ILIKE ‘10.0.%’)) 
  ORDER BY created_at DESC LIMIT 1
) t1&lt;/p&gt;

&lt;p&gt;explain&lt;/p&gt;

&lt;p&gt;SELECT
  (SELECT max(“t2”.”created_at”)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”resource_type” = ‘tokens’) AND
           (“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’)) AND
          NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)))                                                                                                                                                                                                                           AS last_login_at,
  (SELECT “t2”.”client”
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”resource_type” = ‘tokens’) AND
           (“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’)) AND
          NOT (“t2”.”remote_address” ILIKE ‘10.0.%’))
   ORDER BY “t2”.”created_at” DESC
   LIMIT 1)                                                                                                                                                                                                                                                                      AS last_browser_used,
  (SELECT count(‘&lt;em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (((“t2”.”resource_type” = ‘patients’) AND (“t2”.”action” = ‘read’))
           AND (“t2”.”data” ILIKE
                ‘%ref%’))))                                                                                                                                                                                                                                                      AS opened_reports_count,
  (SELECT count(‘&lt;/em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          ((“t2”.”resource_type” = ‘click_resource’) AND (“t2”.”action” =
                                                          ‘created’))))                                                                                                                                                                                                          AS opened_library_count,
  (SELECT count(‘&lt;em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” IN
           (‘click_print_preview_eligible_letter’, ‘click_manage_interventions’, ‘click_report_compare_plans’, ‘click_opportunity_reach_out’, ‘click_opportunity_manage_drugs’, ‘click_opportunity_review_plans’, ‘click_opportunity_view_profile’, ‘click_schedule_message’)))) AS report_actions_count,
  (SELECT count(‘&lt;/em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” =
           ‘click_print_letter’)))                                                                                                                                                                                                                                               AS print_letter_count,
  (SELECT count(‘&lt;em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” =
           ‘click_email_letters’)))                                                                                                                                                                                                                                              AS email_letters_count,
  (SELECT count(‘&lt;/em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” =
           ‘click_schedule_call’)))                                                                                                                                                                                                                                              AS schedule_call_count,
  (SELECT count(‘&lt;em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” =
           ‘click_schedule_calls’)))                                                                                                                                                                                                                                             AS schedule_calls_count,
  (SELECT count(‘&lt;/em&gt;’)
   FROM “request_stats_2015” AS t2
   WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
           NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
          (“t2”.”resource_type” =
           ‘click_opportunity’)))                                                                                                                                                                                                                                                AS opportunity_count,
  ((SELECT count(DISTINCT (“t2”.”resource_id”))
    FROM “request_stats_2015” AS t2
    WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
            NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
           (“t2”.”resource_type” = ‘click_print_preview_plans’))) +
   (SELECT count(DISTINCT (“t2”.”resource_id”))
    FROM “request_stats_2015” AS t2
    WHERE (((“t2”.”store_id” = ‘7f419e906bfe578faef5277c7fc9fb80’) AND
            NOT (“t2”.”remote_address” ILIKE ‘10.0.%’)) AND
           (“t2”.”resource_type” =
            ‘click_print_preview_comparison’))))                                                                                                                                                                                                                                 AS printed_comparisons_count
FROM (SELECT 1) AS void;&lt;/p&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/redshift</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/redshift</guid>
                <pubDate>Mon, 01 Aug 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>PgCrypto overhead comparison with pgbench.</title>
                <description>&lt;p&gt;There are certain scenarios where using certain features (LISTEN/NOTIFY &lt;a href=&quot;http://johtopg.blogspot.com.ar/2015/11/listening-connections-arent-cheap.html&quot;&gt;isn’t cheap&lt;/a&gt;) and contribs (like pgcrypto due to encryption libraries) that consume an extra amount of CPU processing that you may want to be aware. Certainly, processing scale-up isn’t limited to CPUs (known as homogenous scaling): &lt;em&gt;heterogenous scaling up&lt;/em&gt; with GPUs is implemented in &lt;a href=&quot;https://wiki.postgresql.org/wiki/PGStrom&quot;&gt;PG-Strom&lt;/a&gt;. So the answer will depend your requirements.
http://dba.stackexchange.com/questions/17092/is-the-cpu-performance-relevant-for-a-database-server&lt;/p&gt;

&lt;p&gt;Hey there again! This post will be covering simple benchmarks for &lt;a href=&quot;https://www.postgresql.org/docs/9.5/static/pgcrypto.html&quot;&gt;PgCrypto&lt;/a&gt; under
Postgres 9.5.2. As a side note, here there are a few details about the test:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I’m using &lt;a href=&quot;https://hub.docker.com/_/postgres/&quot;&gt;Docker image&lt;/a&gt; – &lt;a href=&quot;https://github.com/docker-library/postgres/blob/master/9.5/Dockerfile&quot;&gt;Code&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;All the tests are in a local machine, and I’ll be focusing in the performance difference
 between algorithms.&lt;/li&gt;
  &lt;li&gt;The test is intended to calculate before hand, the encryption overhead inside
a Postgres instance.&lt;/li&gt;
  &lt;li&gt;I’ll be using custom pgbench scripts in order to have a standard measurement. This
means that in order to  reproduce the tests, you don’t need any additional tool
outside the Postgres ecosystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 1:
As Docker is a new technology, I’ll be covering some basic aspects for those people
whom never played with it before. You can skip those if you already have hands on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 2:
Contribs are included in the Docker image. If you want to run tests over a custom
installation, be aware to ahve installed the contrib packages.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 3:
ALl the code is present at &lt;a href=&quot;https://github.com/3manuek/pgCryptoBench&quot;&gt;pgCryptoBench&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 4:
Sar graphs done with ksar.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;supported-algorithms&quot;&gt;Supported algorithms&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;AES&lt;/a&gt;
&lt;a href=&quot;&quot;&gt;Blowfish&lt;/a&gt;
&lt;a href=&quot;&quot;&gt;3DES&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cpu-basic-benchmark&quot;&gt;CPU basic benchmark&lt;/h2&gt;

&lt;p&gt;There is a way to do a quick benchmark using only encrypt/decrypt functions, without
incurring in the &lt;code class=&quot;highlighter-rouge&quot;&gt;pgbench&lt;/code&gt; usage. This is easier as no previous data is required to
run it. &lt;a href=&quot;https://github.com/3manuek/pgCryptoBench&quot;&gt;pgCryptoBench&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This test runs a process per core and times the execution, no disk activity, only
in-memory processing.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -P postgres
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|sync;
sysctrl -w vm.drop_caches=3;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/pgcryptoppgbench</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/pgbench-crypto-overhead</guid>
                <pubDate>Mon, 01 Aug 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Cassandrastart</title>
                <description>
&lt;p&gt;Read  https://aphyr.com/posts/294-jepsen-cassandra&lt;/p&gt;

&lt;p&gt;https://aphyr.com/posts/293-call-me-maybe-kafka&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm create test -v 2.2.7 -n 3 -s
17:53:45,633 ccm INFO Downloading http://archive.apache.org/dist/cassandra/2.2.7/apache-cassandra-2.2.7-bin.tar.gz to /tmp/ccm-mORIZf.tar.gz (28.229MB)
  29401088  [9917:56:29,230 ccm INFO Extracting /tmp/ccm-mORIZf.tar.gz as version 2.2.7 …
  29600429  [100.00%]Current cluster is now: test&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ sudo ifconfig lo:2 127.0.0.3&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm node1 start
  Traceback (most recent call last):
    File “/usr/local/bin/ccm”, line 5, in &lt;module&gt;
      pkg_resources.run_script('ccm==2.1.11', 'ccm')
    File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 726, in run_script
      self.require(requires)[0].run_script(script_name, ns)
    File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 1491, in run_script
      exec(script_code, namespace, namespace)
    File &quot;/usr/local/lib/python2.7/dist-packages/ccm-2.1.11-py2.7.egg/EGG-INFO/scripts/ccm&quot;, line 86, in &lt;module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;File &quot;build/bdist.linux-x86_64/egg/ccmlib/cmds/node_cmds.py&quot;, line 206, in run
File &quot;build/bdist.linux-x86_64/egg/ccmlib/node.py&quot;, line 542, in start
File &quot;build/bdist.linux-x86_64/egg/ccmlib/common.py&quot;, line 442, in check_socket_available   ccmlib.common.UnavailableSocketError: Inet address 127.0.0.1:9042 is not available: [Errno 98] Address already in use
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm node1 show
  node1: DOWN (Not initialized)
         cluster=test
         auto_bootstrap=False
         thrift=(‘127.0.0.1’, 9160)
         binary=(‘127.0.0.1’, 9042)
         storage=(‘127.0.0.1’, 7000)
         jmx_port=7100
         remote_debug_port=0
         byteman_port=0
         initial_token=-9223372036854775808&lt;/p&gt;

&lt;p&gt;https://github.com/pcmanus/ccm/issues/87
cat ~/bin/loop_alias.sh
#!/bin/bash&lt;/p&gt;

&lt;p&gt;sudo ifconfig lo0 alias 127.0.0.2 up
sudo ifconfig lo0 alias 127.0.0.3 up
sudo ifconfig lo0 alias 127.0.0.4 up
sudo ifconfig lo0 alias 127.0.0.5 up
sudo ifconfig lo0 alias 127.0.0.6 up&lt;/p&gt;

&lt;p&gt;sudo ifconfig lo0 alias 127.0.1.1 up
sudo ifconfig lo0 alias 127.0.1.2 up
sudo ifconfig lo0 alias 127.0.1.3 up
sudo ifconfig lo0 alias 127.0.1.4 up
sudo ifconfig lo0 alias 127.0.1.5 up
sudo ifconfig lo0 alias 127.0.1.6 up&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ps -ef | grep java
emanuel  11317     1  2 17:56 pts/9    00:00:31 java -ea -javaagent:/home/emanuel/.ccm/repository/2.2.7/lib/jamm-0.3.0.jar -XX:+CMSClassUnloadingEnabled -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms500M -Xmx500M -Xmn50M -XX:+HeapDumpOnOutOfMemoryError -Xss256k -XX:StringTableSize=1000003 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseTLAB -XX:+PerfDisableSharedMem -XX:CompileCommandFile=/home/emanuel/.ccm/test/node1/conf/hotspot_compiler -XX:CMSWaitDuration=10000 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSEdenChunksRecordAlways -XX:CMSWaitDuration=10000 -XX:+UseCondCardMark -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -Xloggc:/home/emanuel/.ccm/test/node1/logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -Xloggc:/home/emanuel/.ccm/test/node1/logs/gc.log -Djava.net.preferIPv4Stack=true -Dcassandra.jmx.local.port=7100 -XX:+DisableExplicitGC -Djava.library.path=/home/emanuel/.ccm/repository/2.2.7/lib/sigar-bin -Dcassandra.libjemalloc=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1 -Dlogback.configurationFile=logback.xml -Dcassandra.logdir=/home/emanuel/.ccm/repository/2.2.7/logs -Dcassandra.storagedir=/home/emanuel/.ccm/repository/2.2.7/data -Dcassandra-pidfile=/home/emanuel/.ccm/test/node1/cassandra.pid -cp /home/emanuel/.ccm/test/node1/conf:/home/emanuel/.ccm/repository/2.2.7/build/classes/main:/home/emanuel/.ccm/repository/2.2.7/build/classes/thrift:/home/emanuel/.ccm/repository/2.2.7/lib/ST4-4.0.8.jar:/home/emanuel/.ccm/repository/2.2.7/lib/airline-0.6.jar:/home/emanuel/.ccm/repository/2.2.7/lib/antlr-runtime-3.5.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/apache-cassandra-2.2.7.jar:/home/emanuel/.ccm/repository/2.2.7/lib/apache-cassandra-clientutil-2.2.7.jar:/home/emanuel/.ccm/repository/2.2.7/lib/apache-cassandra-thrift-2.2.7.jar:/home/emanuel/.ccm/repository/2.2.7/lib/cassandra-driver-core-2.2.0-rc2-SNAPSHOT-20150617-shaded.jar:/home/emanuel/.ccm/repository/2.2.7/lib/commons-cli-1.1.jar:/home/emanuel/.ccm/repository/2.2.7/lib/commons-codec-1.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/commons-lang3-3.1.jar:/home/emanuel/.ccm/repository/2.2.7/lib/commons-math3-3.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/compress-lzf-0.8.4.jar:/home/emanuel/.ccm/repository/2.2.7/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/emanuel/.ccm/repository/2.2.7/lib/crc32ex-0.1.1.jar:/home/emanuel/.ccm/repository/2.2.7/lib/disruptor-3.0.1.jar:/home/emanuel/.ccm/repository/2.2.7/lib/ecj-4.4.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/guava-16.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/high-scale-lib-1.0.6.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jackson-core-asl-1.9.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jackson-mapper-asl-1.9.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jamm-0.3.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/javax.inject.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jbcrypt-0.3m.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jcl-over-slf4j-1.7.7.jar:/home/emanuel/.ccm/repository/2.2.7/lib/jna-4.0.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/joda-time-2.4.jar:/home/emanuel/.ccm/repository/2.2.7/lib/json-simple-1.1.jar:/home/emanuel/.ccm/repository/2.2.7/lib/libthrift-0.9.2.jar:/home/emanuel/.ccm/repository/2.2.7/lib/log4j-over-slf4j-1.7.7.jar:/home/emanuel/.ccm/repository/2.2.7/lib/logback-classic-1.1.3.jar:/home/emanuel/.ccm/repository/2.2.7/lib/logback-core-1.1.3.jar:/home/emanuel/.ccm/repository/2.2.7/lib/lz4-1.3.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/metrics-core-3.1.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/metrics-logback-3.1.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/netty-all-4.0.23.Final.jar:/home/emanuel/.ccm/repository/2.2.7/lib/ohc-core-0.3.4.jar:/home/emanuel/.ccm/repository/2.2.7/lib/ohc-core-j8-0.3.4.jar:/home/emanuel/.ccm/repository/2.2.7/lib/reporter-config-base-3.0.0.jar:/home/emanuel/.ccm/repository/2.2.7/lib/reporter-config3
emanuel  12645 10860  0 18:15 pts/9    00:00:00 grep –color=auto –exclude-dir=.bzr –exclude-dir=CVS –exclude-dir=.git –exclude-dir=.hg –exclude-dir=.svn java
➜  ccm git:(master) ✗ kill -9 11317&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm status
Cluster: ‘test’
—————
node1: UP
node3: UP
node2: UP&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm node1 ring&lt;/p&gt;

&lt;h1 id=&quot;datacenter-datacenter1&quot;&gt;Datacenter: datacenter1&lt;/h1&gt;
&lt;p&gt;Address    Rack        Status State   Load            Owns                Token                                     &lt;br /&gt;
                                                                          3074457345618258602                       &lt;br /&gt;
127.0.0.1  rack1       Up     Normal  121.87 KB       66.67%              -9223372036854775808                      &lt;br /&gt;
127.0.0.2  rack1       Up     Normal  95.01 KB        66.67%              -3074457345618258603                      &lt;br /&gt;
127.0.0.3  rack1       Up     Normal  115.57 KB       66.67%              3074457345618258602&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm node2 cqlsh
Connected to test at 127.0.0.2:9042.
[cqlsh 5.0.1 | Cassandra 2.2.7 | CQL spec 3.3.1 | Native protocol v4]
Use HELP for help.
cqlsh&amp;gt; help&lt;/p&gt;

&lt;h1 id=&quot;documented-shell-commands&quot;&gt;Documented shell commands:&lt;/h1&gt;
&lt;p&gt;CAPTURE  CLS          COPY  DESCRIBE  EXPAND  LOGIN   SERIAL  SOURCE   UNICODE
CLEAR    CONSISTENCY  DESC  EXIT      HELP    PAGING  SHOW    TRACING&lt;/p&gt;

&lt;h1 id=&quot;cql-help-topics&quot;&gt;CQL help topics:&lt;/h1&gt;
&lt;p&gt;AGGREGATES        CREATE_COLUMNFAMILY  DROP_INDEX     LIST_PERMISSIONS  UUID
ALTER_KEYSPACE    CREATE_FUNCTION      DROP_KEYSPACE  LIST_USERS    &lt;br /&gt;
ALTER_TABLE       CREATE_INDEX         DROP_TABLE     PERMISSIONS   &lt;br /&gt;
ALTER_TYPE        CREATE_KEYSPACE      DROP_TRIGGER   REVOKE        &lt;br /&gt;
ALTER_USER        CREATE_TABLE         DROP_TYPE      SELECT        &lt;br /&gt;
APPLY             CREATE_TRIGGER       DROP_USER      SELECT_JSON   &lt;br /&gt;
ASCII             CREATE_TYPE          FUNCTIONS      TEXT          &lt;br /&gt;
BATCH             CREATE_USER          GRANT          TIME          &lt;br /&gt;
BEGIN             DATE                 INSERT         TIMESTAMP     &lt;br /&gt;
BLOB              DELETE               INSERT_JSON    TRUNCATE      &lt;br /&gt;
BOOLEAN           DROP_AGGREGATE       INT            TYPES         &lt;br /&gt;
COUNTER           DROP_COLUMNFAMILY    JSON           UPDATE        &lt;br /&gt;
CREATE_AGGREGATE  DROP_FUNCTION        KEYWORDS       USE&lt;/p&gt;

&lt;p&gt;cqlsh&amp;gt; CREATE KEYSPACE mykeyspace
   … WITH REPLICATION = { ‘class’ : ‘SimpleStrategy’, ‘replication_factor’ : 3 };
cqlsh&amp;gt; USE mykeyspave
   … ;
InvalidRequest: code=2200 [Invalid query] message=”Keyspace ‘mykeyspave’ does not exist”
cqlsh&amp;gt; USE mykeyspace
   …
   … ;
cqlsh:mykeyspace&amp;gt; CREATE TABLE users (
              …   user_id int PRIMARY KEY,
              …   fname text,
              …   lname text
              … );
cqlsh:mykeyspace&amp;gt; INSERT INTO users (user_id,  fname, lname)
              …   VALUES (1745, ‘john’, ‘smith’);
cqlsh:mykeyspace&amp;gt; INSERT INTO users (user_id,  fname, lname)
              …   VALUES (1744, ‘john’, ‘doe’);
cqlsh:mykeyspace&amp;gt; INSERT INTO users (user_id,  fname, lname)
              …   VALUES (1746, ‘john’, ‘smith’);
cqlsh:mykeyspace&amp;gt; select * from users;&lt;/p&gt;

&lt;p&gt;user_id | fname | lname
———+——-+——-
    1745 |  john | smith
    1744 |  john |   doe
    1746 |  john | smith&lt;/p&gt;

&lt;p&gt;(3 rows)
cqlsh:mykeyspace&amp;gt; quit
➜  ccm git:(master) ✗ ccm node2 nodetool status&lt;/p&gt;

&lt;h1 id=&quot;datacenter-datacenter1-1&quot;&gt;Datacenter: datacenter1&lt;/h1&gt;
&lt;p&gt;Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
–  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  126.42 KB  1            100.0%            801233ec-95e3-44ce-a009-1cac60a983aa  rack1
UN  127.0.0.2  160.72 KB  1            100.0%            26dd7a0c-6d54-403c-91db-9c09bd5f1988  rack1
UN  127.0.0.3  118.03 KB  1            100.0%            16c35fa9-26e5-4e82-b2fd-c9d22a4f944e  rack1&lt;/p&gt;

&lt;p&gt;➜  ccm git:(master) ✗ ccm node1 nodetool status&lt;/p&gt;

&lt;h1 id=&quot;datacenter-datacenter1-2&quot;&gt;Datacenter: datacenter1&lt;/h1&gt;
&lt;p&gt;Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
–  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  126.42 KB  1            100.0%            801233ec-95e3-44ce-a009-1cac60a983aa  rack1
UN  127.0.0.2  160.72 KB  1            100.0%            26dd7a0c-6d54-403c-91db-9c09bd5f1988  rack1
UN  127.0.0.3  118.03 KB  1            100.0%            16c35fa9-26e5-4e82-b2fd-c9d22a4f944e  rack1&lt;/p&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/2016/07/12/cassandraStart</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/2016/07/12/cassandraStart</guid>
                <pubDate>Tue, 12 Jul 2016 23:34:19 -0300</pubDate>
        </item>

        <item>
                <title>HOWTO Percona Server with docker</title>
                <description>&lt;h2 id=&quot;before-starting-the-container&quot;&gt;Before starting the container&lt;/h2&gt;

&lt;p&gt;This article is not an introductory explanation of docker,however it’s scope if for docker’s beginners. You can consider it as an extension of the well documented &lt;a href=&quot;https://hub.docker.com/_/percona/&quot;&gt;Percona docker hub doc&lt;/a&gt;. For the source code of the image, the repository is at &lt;a href=&quot;https://github.com/dockerfile/percona&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is the all what you need to do for start:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run --name percona57 -e &lt;span class=&quot;nv&quot;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;a_password&amp;gt;  -d percona:5.7
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For checking the container status log, you can execute &lt;code class=&quot;highlighter-rouge&quot;&gt;docker logs percona57&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;additional-mysql-logs&quot;&gt;Additional MySQL logs&lt;/h2&gt;

&lt;p&gt;To start the container is pretty easy, but if you are not very used to Docker, you will find a bit lost if you want to enable logging or other features.&lt;/p&gt;

&lt;p&gt;For example, a full logging container will be started with this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run --name percona57  -v /var/log/mysql:/var/log/mysql  -e MYSQL_ROOT_PASSWORD=mysql  -d percona:5.7 --general-log=1 --slow-query-log=1 --long-query-time=0  --log_slow_verbosity='full, profiling, profiling_use_getrusage'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;log_slow_verbosity&lt;/code&gt; is only applicable for the Percona release, and adds extra output that turns very useful when doing complex query reviews. As you can appreciate, all the options are passed after the image name (percona:5.7).&lt;/p&gt;

&lt;p&gt;Now, the question is: where are the logs? Generally, you can access the container using &lt;code class=&quot;highlighter-rouge&quot;&gt;docker exec -it percona57 bash&lt;/code&gt; and view the logs inside it, although this is not the most comfortable way to do this.&lt;/p&gt;

&lt;p&gt;In the example bellow, we’ll use &lt;code class=&quot;highlighter-rouge&quot;&gt;jq&lt;/code&gt; (a very handy json parser).&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3laptop ~ &lt;span class=&quot;c&quot;&gt;# docker ps&lt;/span&gt;
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
cb740be0743c        percona:5.7         &lt;span class=&quot;s2&quot;&gt;&quot;docker-entrypoint.sh&quot;&lt;/span&gt;   35 minutes ago      Up 35 minutes       3306/tcp            percona57

3laptop ~ &lt;span class=&quot;c&quot;&gt;# docker inspect percona57 | jq .[].Mounts&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;Propagation&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;rprivate&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;RW&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Mode&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Destination&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/var/log/mysql&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Source&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/var/log/mysql&quot;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
  &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;Propagation&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;RW&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Mode&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Driver&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;local&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Destination&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/var/lib/mysql&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Source&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/var/lib/docker/volumes/ceda51de62dac317fcafe9dd9e8f9b6f1dc5d70874466b3faf7cdfbcbbc91154/_data&quot;&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;ceda51de62dac317fcafe9dd9e8f9b6f1dc5d70874466b3faf7cdfbcbbc91154&quot;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

3laptop ~ &lt;span class=&quot;c&quot;&gt;# ls -l /var/lib/docker/volumes/ceda51de62dac317fcafe9dd9e8f9b6f1dc5d70874466b3faf7cdfbcbbc91154/_data&lt;/span&gt;
...
-rw-r----- 1 maxscale docker  26886023 Jul  2 19:10 cb740be0743c.log
-rw-r----- 1 maxscale docker 268834670 Jul  2 19:10 cb740be0743c-slow.log
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The logs (general and slow) are using the &lt;code class=&quot;highlighter-rouge&quot;&gt;container id&lt;/code&gt; in the file name, which can be appreciated when executing &lt;code class=&quot;highlighter-rouge&quot;&gt;docker ps&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;access-through-network&quot;&gt;Access through network&lt;/h2&gt;

&lt;p&gt;Obviously, when using docker in production, you don’t want to access it locally.  For getting the host of our container (and all the running containers), we can do the following commands:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3laptop ~ # docker network ls
NETWORK ID          NAME                DRIVER
5fddd2e1a80a        bridge              bridge              
e4e0c655e1aa        host                host                
565f4a23d95a        none                null  

3laptop ~ # docker network inspect 5fddd2e1a80a | jq .[].Containers
{
  &quot;cb740be0743cd662c700f73586fe481dc25e4eb27ef94e075c4668a5421eca13&quot;: {
    &quot;IPv6Address&quot;: &quot;&quot;,
    &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;,
    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;,
    &quot;EndpointID&quot;: &quot;6dbd28900efe2c6f6edffcbbec0ac7d6446b4336e6e31f018f18d00f1005a812&quot;,
    &quot;Name&quot;: &quot;percona57&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We can see that our container &lt;code class=&quot;highlighter-rouge&quot;&gt;percona57&lt;/code&gt; is running over &lt;code class=&quot;highlighter-rouge&quot;&gt;172.17.0.2&lt;/code&gt; IP address. To access it, you only need to do as usual:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3laptop ~ # mysql -h 172.17.0.2 -p
....
mysql&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/perconaserverbasichowto</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/percona-docker</guid>
                <pubDate>Sat, 02 Jul 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Jekyll Introduction Draft</title>
                <description>
&lt;p&gt;This is an example of a draft. Read more here: &lt;a href=&quot;http://jekyllrb.com/docs/drafts/&quot;&gt;http://jekyllrb.com/docs/drafts/&lt;/a&gt;&lt;/p&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/lessons/2016/06/22/jekyll-introduction-draft</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/lessons/2016/06/22/jekyll-introduction-draft</guid>
                <pubDate>Wed, 22 Jun 2016 15:29:04 -0300</pubDate>
        </item>

        <item>
                <title>Keeping persistent connections in Postgres.</title>
                <description>&lt;p&gt;It sounds like a bad idea, I won’t disagree,  however there is certainly usual that
some applications use persistent connections, without handling the &lt;code class=&quot;highlighter-rouge&quot;&gt;keepalive&lt;/code&gt;.
Before to spit out the &lt;em&gt;partial workaround&lt;/em&gt; (assuming that you don’t have any firewall
or network setup), you should read this &lt;a href=&quot;http://hans.io/blog/2014/02/19/postgresql_connection/&quot;&gt;article&lt;/a&gt; which actually shows how bad is
the idea of keeping connections open in certain scenarios.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;potato&lt;/em&gt; in Postgres is to play with the &lt;a href=&quot;https://www.postgresql.org/docs/9.5/static/runtime-config-connection.html#GUC-TCP-KEEPALIVES-IDLE&quot;&gt;&lt;em&gt;tcp options&lt;/em&gt;&lt;/a&gt;,
which I know, it isn’t the most clear way to say &lt;em&gt;idle&lt;/em&gt; with &lt;em&gt;timeout&lt;/em&gt; together.&lt;/p&gt;

&lt;p&gt;However it has a point, as it is actually how it works, and it allows you to play
with the same variables as most of the network solutions that follow the TCP protocol.&lt;/p&gt;

&lt;p&gt;Let’s explain the TCP keepalives in a &lt;em&gt;street lang&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;tcp_keepalives_idle server -&amp;gt; client each n seconds
tcp_keepalives_interval server’s nap after the last NOT ACKED keepalive message from the client.
tcp_keepalives_count is basically, the server is asking himself “how many times should I ignore?”.&lt;/p&gt;

&lt;p&gt;What we don’t want is to keep alive sessions that get closed by an abortion signal.&lt;/p&gt;

&lt;p&gt;tcp_keepalives_interval
tcp_keepalives_idle en 36000. 10 horas.&lt;/p&gt;

&lt;p&gt;tcp_keepalives_count = 1 (exacto 10 horas) , n (numero de counts,cada uno 10 horas)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@658e7f4e9bff:/var/lib/postgresql/data# sysctl -A |grep -i tcp                 
fs.nfs.nlm_tcpport = 0
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_fwmark_accept = 0
net.netfilter.nf_conntrack_tcp_be_liberal = 0
net.netfilter.nf_conntrack_tcp_loose = 1
net.netfilter.nf_conntrack_tcp_max_retrans = 3
net.netfilter.nf_conntrack_tcp_timeout_close = 10
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60
net.netfilter.nf_conntrack_tcp_timeout_established = 432000
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120
net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30
net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300
net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60
net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120
net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300
sunrpc.tcp_fin_timeout = 15
sunrpc.tcp_max_slot_table_entries = 65536
sunrpc.tcp_slot_table_entries = 2
sunrpc.transports = tcp 1048576
sunrpc.transports = tcp-bc 1048576


root@658e7f4e9bff:/var/lib/postgresql/data# cat /proc/net/tcp
  sl  local_address rem_address   st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt   uid  timeout inode                                                     
   0: 00000000:1538 00000000:0000 0A 00000000:00000000 00:00000000 00000000   999        0 24653 1 0000000000000000 100 0 0 10 0                     
   1: 020011AC:1538 010011AC:D814 01 00000000:00000000 02:0000003F 00000000   999        0 29752 2 0000000000000000 20 4 1 10 -1                     
root@658e7f4e9bff:/var/lib/postgresql/data# cat /proc/net/sockstat
sockets: used 544
TCP: inuse 2 orphan 0 tw 2 alloc 30 mem 1
UDP: inuse 0 mem 10
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0
root@658e7f4e9bff:/var/lib/postgresql/data# cat /proc/net/tcp     
  sl  local_address rem_address   st tx_queue rx_queue tr tm-&amp;gt;when retrnsmt   uid  timeout inode                                                     
   0: 00000000:1538 00000000:0000 0A 00000000:00000000 00:00000000 00000000   999        0 24653 1 0000000000000000 100 0 0 10 0                     
   1: 020011AC:1538 010011AC:D814 01 00000000:00000000 02:0000001A 00000000   999        0 29752 2 0000000000000000 20 4 1 10 -1                     
   2: 020011AC:1538 010011AC:D840 01 00000000:00000000 02:0000002C 00000000   999        0 37341 2 0000000000000000 20 4 23 10 -1                    
root@658e7f4e9bff:/var/lib/postgresql/data# cat /proc/net/sockstat
sockets: used 546
TCP: inuse 3 orphan 0 tw 3 alloc 32 mem 1
UDP: inuse 0 mem 10
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0

root@658e7f4e9bff:/var/lib/postgresql/data# sysctl -A |grep -i ^net.ipv4.conf                
net.ipv4.conf.all.accept_local = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.all.arp_accept = 0
net.ipv4.conf.all.arp_announce = 0
net.ipv4.conf.all.arp_filter = 0
net.ipv4.conf.all.arp_ignore = 0
net.ipv4.conf.all.arp_notify = 0
net.ipv4.conf.all.bootp_relay = 0
net.ipv4.conf.all.disable_policy = 0
net.ipv4.conf.all.disable_xfrm = 0
net.ipv4.conf.all.force_igmp_version = 0
net.ipv4.conf.all.forwarding = 1
net.ipv4.conf.all.igmpv2_unsolicited_report_interval = 10000
net.ipv4.conf.all.igmpv3_unsolicited_report_interval = 1000
net.ipv4.conf.all.log_martians = 0
net.ipv4.conf.all.mc_forwarding = 0
net.ipv4.conf.all.medium_id = 0
net.ipv4.conf.all.promote_secondaries = 0
net.ipv4.conf.all.proxy_arp = 0
net.ipv4.conf.all.proxy_arp_pvlan = 0
net.ipv4.conf.all.route_localnet = 0
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.all.secure_redirects = 1
net.ipv4.conf.all.send_redirects = 1
net.ipv4.conf.all.shared_media = 1
net.ipv4.conf.all.src_valid_mark = 0
net.ipv4.conf.all.tag = 0
net.ipv4.conf.default.accept_local = 0
net.ipv4.conf.default.accept_redirects = 1
net.ipv4.conf.default.accept_source_route = 1
net.ipv4.conf.default.arp_accept = 0
net.ipv4.conf.default.arp_announce = 0
net.ipv4.conf.default.arp_filter = 0
net.ipv4.conf.default.arp_ignore = 0
net.ipv4.conf.default.arp_notify = 0
net.ipv4.conf.default.bootp_relay = 0
net.ipv4.conf.default.disable_policy = 0
net.ipv4.conf.default.disable_xfrm = 0
net.ipv4.conf.default.force_igmp_version = 0
net.ipv4.conf.default.forwarding = 1
net.ipv4.conf.default.igmpv2_unsolicited_report_interval = 10000
net.ipv4.conf.default.igmpv3_unsolicited_report_interval = 1000
net.ipv4.conf.default.log_martians = 0
net.ipv4.conf.default.mc_forwarding = 0
net.ipv4.conf.default.medium_id = 0
net.ipv4.conf.default.promote_secondaries = 0
net.ipv4.conf.default.proxy_arp = 0
net.ipv4.conf.default.proxy_arp_pvlan = 0
net.ipv4.conf.default.route_localnet = 0
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.secure_redirects = 1
net.ipv4.conf.default.send_redirects = 1
net.ipv4.conf.default.shared_media = 1
net.ipv4.conf.default.src_valid_mark = 0
net.ipv4.conf.default.tag = 0
net.ipv4.conf.eth0.accept_local = 0
net.ipv4.conf.eth0.accept_redirects = 1
net.ipv4.conf.eth0.accept_source_route = 1
net.ipv4.conf.eth0.arp_accept = 0
net.ipv4.conf.eth0.arp_announce = 0
net.ipv4.conf.eth0.arp_filter = 0
net.ipv4.conf.eth0.arp_ignore = 0
net.ipv4.conf.eth0.arp_notify = 0
net.ipv4.conf.eth0.bootp_relay = 0
net.ipv4.conf.eth0.disable_policy = 0
net.ipv4.conf.eth0.disable_xfrm = 0
net.ipv4.conf.eth0.force_igmp_version = 0
net.ipv4.conf.eth0.forwarding = 1
net.ipv4.conf.eth0.igmpv2_unsolicited_report_interval = 10000
net.ipv4.conf.eth0.igmpv3_unsolicited_report_interval = 1000
net.ipv4.conf.eth0.log_martians = 0
net.ipv4.conf.eth0.mc_forwarding = 0
net.ipv4.conf.eth0.medium_id = 0
net.ipv4.conf.eth0.promote_secondaries = 0
net.ipv4.conf.eth0.proxy_arp = 0
net.ipv4.conf.eth0.proxy_arp_pvlan = 0
net.ipv4.conf.eth0.route_localnet = 0
net.ipv4.conf.eth0.rp_filter = 1
net.ipv4.conf.eth0.secure_redirects = 1
net.ipv4.conf.eth0.send_redirects = 1
net.ipv4.conf.eth0.shared_media = 1
net.ipv4.conf.eth0.src_valid_mark = 0
net.ipv4.conf.eth0.tag = 0
net.ipv4.conf.lo.accept_local = 0
net.ipv4.conf.lo.accept_redirects = 1
net.ipv4.conf.lo.accept_source_route = 1
net.ipv4.conf.lo.arp_accept = 0
net.ipv4.conf.lo.arp_announce = 0
net.ipv4.conf.lo.arp_filter = 0
net.ipv4.conf.lo.arp_ignore = 0
net.ipv4.conf.lo.arp_notify = 0
net.ipv4.conf.lo.bootp_relay = 0
net.ipv4.conf.lo.disable_policy = 1
net.ipv4.conf.lo.disable_xfrm = 1
net.ipv4.conf.lo.force_igmp_version = 0
net.ipv4.conf.lo.forwarding = 1
net.ipv4.conf.lo.igmpv2_unsolicited_report_interval = 10000
net.ipv4.conf.lo.igmpv3_unsolicited_report_interval = 1000
net.ipv4.conf.lo.log_martians = 0
net.ipv4.conf.lo.mc_forwarding = 0
net.ipv4.conf.lo.medium_id = 0
net.ipv4.conf.lo.promote_secondaries = 0
net.ipv4.conf.lo.proxy_arp = 0
net.ipv4.conf.lo.proxy_arp_pvlan = 0
net.ipv4.conf.lo.route_localnet = 0
net.ipv4.conf.lo.rp_filter = 1
net.ipv4.conf.lo.secure_redirects = 1
net.ipv4.conf.lo.send_redirects = 1
net.ipv4.conf.lo.shared_media = 1
net.ipv4.conf.lo.src_valid_mark = 0
net.ipv4.conf.lo.tag = 0

3laptop ~ # docker network inspect bridge | jq .[].Options
{
  &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;,
  &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;,
  &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;,
  &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,
  &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;,
  &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;
}


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;how-to-run-a-test-with-docker&quot;&gt;How to run a test with Docker&lt;/h2&gt;

&lt;p&gt;You have all you need to start &lt;a href=&quot;https://github.com/docker-library/docs/tree/master/postgres&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In that doc it is not mentioned, but you can pass variables when executing the run:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3laptop ~ # docker run --name postgres95 -e POSTGRES_PASSWORD=postgres -d postgres --tcp_keepalives_idle=3 --tcp_keepalives_interval=2 --tcp_keepalives_count=3
658e7f4e9bff6768dbcc3d3db1d22639d76f4c125e6e571423f23dac6fce031f
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  ~ aws --region=us-east-1 ecs run-task --task-definition postgres:latest
Could not connect to the endpoint URL: &quot;https://ecs.sa-east-1.amazonaws.com/&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/keeppersistentconnections</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/keeping-idle-connections-open</guid>
                <pubDate>Wed, 22 Jun 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>How you can start MySQL Slow query analysis.</title>
                <description>&lt;h2 id=&quot;objective-of-the-article&quot;&gt;Objective of the article&lt;/h2&gt;

&lt;p&gt;This is not intended to be the ultimate guide for query analysis, it is just a simple starting guide for people that want to do so.
If you want to start from something, I strongly recommend you to start with &lt;a href=&quot;https://www.amazon.es/Effective-MySQL-Optimizing-Statements-Oracle/dp/0071782796&quot;&gt;Effective MySQL: optimizing SQL Statements by Ronald Bradford&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Query Analysis is pretty much valuated for consultants, as a good query analysis
and query rewriting can &lt;em&gt;save money&lt;/em&gt;. I saw customers buying new hardware just because &lt;em&gt;the MySQL was too slow&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;And -the most important- is not only about performance. A good query profiling can be a good
diagnostic of the entire software and data architecture. Sometimes, RDBMS are used for stuff
that are not the best fit for or, even NoSQLs were used when MySQL/Postgres can be a better fit.&lt;/p&gt;

&lt;p&gt;Also, this article is not focus on SQL tricks or neither how to understand MySQL
explain, which I assume you already have some knowledge to continue the reading. A
very nice library to enjoy can be found at &lt;a href=&quot;http://use-the-index-luke.com/&quot;&gt;Use the index, Luke!&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-approach&quot;&gt;The approach&lt;/h2&gt;

&lt;p&gt;First of all you need to know what the customer is literally doing in the server,
which information and type of server they have online. This may be something pretty
much &lt;em&gt;obvious&lt;/em&gt;, but believe me, it is not trivial to repeat this.&lt;/p&gt;

&lt;p&gt;A server could be part of a sharded cluster, a report server, a BI server, a web
&lt;em&gt;OLTP&lt;/em&gt; server, test server, and so on. Market impose those infinite combinations,
however there are a few rules for good practice when writing queries. And those
good practices will depend on the RDBMS you are working on (in this particular case,
  we will focus on MySQL).&lt;/p&gt;

&lt;p&gt;Now, you know what you need to get and how to start the analysis. Beyond query complexity,
sometimes you need to know which solution can be applied or not when you rewrite the query or provide suggestions.&lt;/p&gt;

&lt;p&gt;Generally, queries can be slow due to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Missing indexes&lt;/li&gt;
  &lt;li&gt;Bad cardinality and not useful filters&lt;/li&gt;
  &lt;li&gt;Inner joins with outer order using different keys&lt;/li&gt;
  &lt;li&gt;File sorting&lt;/li&gt;
  &lt;li&gt;Bad writing queries (large subqueries, large IN clauses, non targeted bugs, i.e.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check the latest &lt;a href=&quot;https://bugs.mysql.com/search.php?cmd=display&amp;amp;status=Active&amp;amp;severity=-5&amp;amp;search_for=query&amp;amp;os=0&amp;amp;bug_age=0&amp;amp;order_by=bug_type&amp;amp;direction=ASC&amp;amp;limit=10&amp;amp;mine=0&amp;amp;reorder_by=bug_type&quot;&gt;bugs reported&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-much-do-you-need-to-collect&quot;&gt;How much do you need to collect?&lt;/h2&gt;

&lt;p&gt;For a query analysis you want to collect &lt;em&gt;as much as you can&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;general query log, or&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;long_query_time&lt;/code&gt; = 0 or,&lt;/li&gt;
  &lt;li&gt;tcpdump&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using others will lead to non-complete profiling when processing. However, there are cases where is not possible to have the &lt;code class=&quot;highlighter-rouge&quot;&gt;long_query_time = 0&lt;/code&gt; due to the high amount of TPS (&lt;em&gt;Transactions Per Second&lt;/em&gt;). You can set it to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.5&lt;/code&gt; or higher. The closer to 0, the better.&lt;/p&gt;

&lt;p&gt;You will collect the whole set of queries. You are not hunting &lt;em&gt;slow queries&lt;/em&gt; but also &lt;em&gt;very frequent queries&lt;/em&gt;. I did have cases where the issue was not regarding any slow query, but an application bug doing 2x the same query. Also you will be probably hunting buggy queries, with unnecessary large result sets, suspicious orders, bad FTS usage, slow procedures, etc.&lt;/p&gt;

&lt;p&gt;Generally, when analyzing BI or reporting servers, it is accepted to have a large &lt;code class=&quot;highlighter-rouge&quot;&gt;long_query_time&lt;/code&gt;, as you will probably focusing on slow queries.&lt;/p&gt;

&lt;h3 id=&quot;percona-enhancements-to-be-aware-of&quot;&gt;Percona enhancements to be aware of&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.percona.com/doc/percona-server/5.7/diagnostics/slow_extended.html&quot;&gt;Slow log extended&lt;/a&gt; options add extra verbosity to the slow log.&lt;/p&gt;

&lt;p&gt;The option is &lt;code class=&quot;highlighter-rouge&quot;&gt;log_slow_verbosity&lt;/code&gt; and it has several options. Just keep in mind that &lt;code class=&quot;highlighter-rouge&quot;&gt;full&lt;/code&gt; does not include &lt;code class=&quot;highlighter-rouge&quot;&gt;profiling&lt;/code&gt; neither &lt;code class=&quot;highlighter-rouge&quot;&gt;profiling_use_getrusage&lt;/code&gt;. I assume that you don’t need to enable profiling just the first time you run a query review. It will interesting to enable the full query collection if using &lt;a href=&quot;https://www.percona.com/doc/percona-playback/index.html&quot;&gt;Percona Playback&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pt-query-digest-is-your-friend&quot;&gt;&lt;a href=&quot;https://www.percona.com/doc/percona-toolkit/2.2/pt-query-digest.html#cmdoption-pt-query-digest--review&quot;&gt;pt-query-digest&lt;/a&gt; is your friend&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pt-query-digest&lt;/code&gt; can be used with an impressive set of options. Before you use it, I encourage the reader to do a quick read over its documentation.&lt;/p&gt;

&lt;p&gt;For example, you can gather not only the report but also the explains of the queries safely (that is, if a query has a subquery, it won’t execute the query on the master).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pt-query-digest --type=slowlog --report-all --explain h=172.17.0.2 --user=root --password=mysql /var/lib/docker/volumes/ceda51de62dac317fcafe9dd9e8f9b6f1dc5d70874466b3faf7cdfbcbbc91154/_data/cb740be0743c-slow.log &amp;gt; /tmp/report.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;examining-the-query-results&quot;&gt;Examining the query results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Rows examined vs. rows returned&lt;/li&gt;
  &lt;li&gt;The order of the result&lt;/li&gt;
  &lt;li&gt;Does the application uses the full set of rows? Limit the number of rows as much as possible&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-complementary-tools&quot;&gt;Other complementary tools&lt;/h2&gt;

&lt;h3 id=&quot;anemometer&quot;&gt;&lt;a href=&quot;https://github.com/box/Anemometer&quot;&gt;Anemometer&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;If you have a large fleet of MySQL servers and you usually do query analysis, the &lt;em&gt;next tool&lt;/em&gt; you want to look is &lt;a href=&quot;https://github.com/3manuek/Anemometer&quot;&gt;Anemometer&lt;/a&gt;.Originally, this project has been made by Box, however if you want to test the vagrant machine, I suggest to use the fork linked above. The project looks like stable and they are not merging new pull requests. It just works.&lt;/p&gt;

&lt;p&gt;The idea was to have available in a single glance the slow logs, which can become very handy when scaling complex boxes. Also it improves proactive monitoring and partial trending.&lt;/p&gt;

&lt;h3 id=&quot;binlogeventstats&quot;&gt;&lt;a href=&quot;https://github.com/pythian/binlogEventStats&quot;&gt;binlogEventStats&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The idea is to do a &lt;em&gt;top style&lt;/em&gt; metrics of the  streamed transactions from the replication flow in a more detailed way, so you can see the writes from a master (ideally to trace/debug slave lags). This is not entirely related with Query Reviews generally, however it could be a detection tool when some unexpected floods happen.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: Is still in development.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-main-parts-of-the-pt-query-digest&quot;&gt;The main parts of the &lt;code class=&quot;highlighter-rouge&quot;&gt;pt-query-digest&lt;/code&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Overall stats (useful for general comparisons)&lt;/li&gt;
  &lt;li&gt;Profile: Query by ranking in terms of execution time.&lt;/li&gt;
  &lt;li&gt;Queries: Each query with execution details.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;where-do-i-start-analyzing&quot;&gt;Where do I start analyzing?&lt;/h3&gt;

&lt;p&gt;The general rule of the thumb is start by the heaviest down to the others. I would
say that it depends on how much time you want to waste and the complexity of the queries.&lt;/p&gt;

&lt;p&gt;My recommendation is to do the queries that consume more than 40-50% of accumulated execution time on BI servers
and 50-70% when it is an OLTP workload.&lt;/p&gt;

&lt;p&gt;Once you do a first query review, the subsequent analysis will not be very useful if no
changes are applied on the queries.&lt;/p&gt;

&lt;h3 id=&quot;get-the-table-details&quot;&gt;Get the table details&lt;/h3&gt;

&lt;p&gt;How to execute the SHOWS in the &lt;code class=&quot;highlighter-rouge&quot;&gt;pt-query-digest&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;egrep &lt;span class=&quot;s2&quot;&gt;&quot;SHOW.[TABLE|CREATE].*&quot;&lt;/span&gt; /tmp/report.txt | sed &lt;span class=&quot;s1&quot;&gt;'s/^#\s*//'&lt;/span&gt; | sed &lt;span class=&quot;s1&quot;&gt;'s/\\/\\\\/g'&lt;/span&gt; | sort | uniq | sed &lt;span class=&quot;s2&quot;&gt;&quot;s/'/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\\'&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/g&quot;&lt;/span&gt; | xargs -i mysql --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mysql --password&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;SHADOW -e &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &amp;gt; /tmp/SHOW.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As a rule, the rewritten query should return the same amount of records and order, unless
your recommendation specifies that the current result set size or order are not
convenient. i.e. very large result sets with a lot of discarded rows from the application,
a query that is returning an incorrect order, etc.&lt;/p&gt;

&lt;h2 id=&quot;comparing-the-before-and-after&quot;&gt;Comparing the before and after&lt;/h2&gt;

&lt;p&gt;Finally, once changes have been made (any change)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Same interval of time and day use ( &lt;code class=&quot;highlighter-rouge&quot;&gt;--since&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;--until&lt;/code&gt; options ).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--review&lt;/code&gt; option will help you to do incremental analysis.&lt;/li&gt;
  &lt;li&gt;TPS before and after.&lt;/li&gt;
  &lt;li&gt;Overall Execution time make easier the job of comparing the effectiveness of the changes.&lt;/li&gt;
  &lt;li&gt;Use always the same &lt;code class=&quot;highlighter-rouge&quot;&gt;long_query_time&lt;/code&gt; on both.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/querydigestcomplements</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/pt-query-digest-usage</guid>
                <pubDate>Wed, 22 Jun 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>MySQL 5.7 InnoDB's Full Text Search overview.</title>
                <description>&lt;p&gt;Main application:
&lt;a href=&quot;https://3manuek.shinyapps.io/FTS_Innodb/&quot;&gt;InnoDB’s Full Text Search overview (with Shiny/R)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;credits&quot;&gt;Credits&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Author: Emanuel Calvo&lt;/p&gt;

  &lt;p&gt;Company: Pythian&lt;/p&gt;

  &lt;p&gt;Thanks to Valerie Parham-Thompson @ Pythian and Daniel Prince @ Oracle.&lt;/p&gt;

  &lt;p&gt;Repository available at &lt;a href=&quot;https://github.com/3manuek/fts_article&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For the whole article and the Shinyapp application is available &lt;a href=&quot;https://3manuek.shinyapps.io/FTS_Innodb/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-initial-thoughts&quot;&gt;Some initial thoughts&lt;/h2&gt;

&lt;p&gt;A couple of days ago one of our customers came up with a question regarding FTS
over InnoDB engine. Although the question is not answered in the current article,
I came up with the conclusion that FTS is sometimes misunderstood.&lt;/p&gt;

&lt;p&gt;The point of this &lt;em&gt;article&lt;/em&gt; is to show dynamically how the search algorithms work,
using non-fictional data (data sources were downloaded from &lt;a href=&quot;www.gutenberg.org&quot;&gt;Gutenberg project&lt;/a&gt;
 ) within an easy interface.&lt;/p&gt;

&lt;p&gt;In order to show the effects off the field sizes over the query expansion algorithm,
you will see two main tables (bookContent and bookContentByLine) both containing
the same books in different approaches: by line and by paragraph. You’ll see the
noise generated by the &lt;code class=&quot;highlighter-rouge&quot;&gt;QUERY EXPANSION&lt;/code&gt; algorithm when phrases are too large.&lt;/p&gt;

&lt;p&gt;The current article has been developed using Shiny/R in order to allow you to see
the effects of the algorithms.&lt;/p&gt;

&lt;p&gt;For the sake of simplicity, in this article we won’t go through the FTS parsers.
Probably that would be material for a future post.&lt;/p&gt;

&lt;h2 id=&quot;why-i-consider-fts-sometimes-misunderstood&quot;&gt;Why I consider FTS sometimes misunderstood?&lt;/h2&gt;

&lt;p&gt;FTS is a technology that can be use for any purpose, not only simple searches.
There is a myth that the FTS only should be placed on clusters for that purpose,
which I agree. However, certain bussines rules require complex searches, and
having such feature can be a win.&lt;/p&gt;

&lt;p&gt;RDBMS aren’t a good place for massive amount of FTS queries, without using any
of the join capabilities that they offer, or the ACID complaints.&lt;/p&gt;

&lt;p&gt;As I said above, FTS is totally acceptable in RDBMS, if you are using at least
one RDBMS critical feature, required by your bussines model.&lt;/p&gt;

&lt;h2 id=&quot;action&quot;&gt;Action!&lt;/h2&gt;

&lt;p&gt;Here is an example of how ranks differ among algorithms and field sizes using the word ‘country’:&lt;/p&gt;

&lt;p&gt;To start showing the effects of the algorithms, the following example searches
the word ‘country’ using &lt;code class=&quot;highlighter-rouge&quot;&gt;query expansion&lt;/code&gt;. This means that we are not looking
only the exact matches, but also the entries that appear the most when the
the exact match has been found.&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT&lt;/code&gt; clause you’ll see both FTS expressions using &lt;code class=&quot;highlighter-rouge&quot;&gt;NATURAL LANGUAGE&lt;/code&gt;
with query expansion and &lt;code class=&quot;highlighter-rouge&quot;&gt;BOOLEAN&lt;/code&gt;modes respectively.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;innodb_ft_aux_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ftslab/bookContentByLine'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NATURAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LANGUAGE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QUERY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXPANSION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QERank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BOOLEAN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BoolRank&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bl&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;join&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;information_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INNODB_FT_INDEX_TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DOC_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
      &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NATURAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LANGUAGE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QUERY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXPANSION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'country'&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------------------------------------------------------------------------+--------+-----+--------+--------+
|content                                                                      | bookid | pos | QERank |BoolRank|
+-----------------------------------------------------------------------------+--------+-----+--------+--------+
|&quot;country in September, migrating into Holland, and leave their mates behind&quot; |  15707 | 1   |    105 |      7 |
|&quot;unsatisfied desire to serve his country, the two prevalent enthusiasms at&quot;  |  15707 | 33  |     98 |      7 |
|&quot;Language, Vol. I. p. 212. In this country, where four or five horses travel&quot;|  15707 | 35  |     93 |      7 |
|&quot;inflicting immense damage upon the country. Whereupon the Florentines&quot;      |   1232 | 36  |     89 |      7 |
|&quot;made for a country of twenty or thirty millionsâ€™ population, can be laid&quot; |  39064 | 12  |     89 |      7 |
|&quot;The spiders of this country manufacture nets of various forms, adapted to&quot;  |  15707 | 21  |     87 |      7 |
|&quot;a velvet-covered arm-chair at my head! This country is too decadent&quot;        |  33415 | 45  |     86 |      7 |
|&quot;country may be ennobled, and under its auspices may be verified that&quot;       |   1232 | 1   |     84 |      7 |
|&quot;name. The writer of this unpublished pamphlet sees his country in a&quot;        |  39064 | 56  |     84 |      7 |
|&quot;In our country, Mr. Pennant informs us, that some quails migrate, and&quot;      |  15707 | 8   |     83 |      7 |
|&quot;all the morning in passing over the adjacent country.&quot; (Voyage to Senegal,&quot; |  15707 | 46  |     82 |      7 |
|&quot;the electoral system of the country. Immediately an outcry burst out&quot;       |  39064 | 29  |     82 |      7 |
|&quot;country, under a most excellent president, wherein all cities had their&quot;    |   1232 | 1   |     81 |      7 |
|&quot;Though in this country horses shew little vestiges of policy, yet in the&quot;   |  15707 | 16  |     81 |      7 |
|&quot;country districts. As Lucca had five gates, he divided his own country&quot;     |   1232 | 1,63|     80 |     14 |
+-----------------------------------------------------------------------------+--------+-----+--------+--------+
15 rows in set (1,16 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The noise generated by the query expansion is expected and described in the official documentation &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/fulltext-query-expansion.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The interesting case is the following row, which has 2 exact occurrences and it is not the highest rank using query
extension. Remember, this is expected.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Text: &quot;country districts. As Lucca had five gates, he divided his own country&quot;
bookid: 1232
pos: 1,63
QERank: 80
BoolRank: 14
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is even worser when using large sentences. In the example bellow you will see the same query, against the table
storing by paragraph. The boolean rank shows some of the entries way above others, however the query extension
locates at the top records that not necessarily has a lot of exact matches.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;innodb_ft_aux_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ftslab/bookContent'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;group_concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NATURAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LANGUAGE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QUERY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXPANSION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QERank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BOOLEAN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BooleanRank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bl&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;join&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;information_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INNODB_FT_INDEX_TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DOC_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
          &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;MATCH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AGAINST&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;country&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NATURAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LANGUAGE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QUERY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXPANSION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;AND&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'country'&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QERank&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DESC&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+------------+-----------------+--------+-------------+-------+
| bookid | FTS_DOC_ID | positions       | QERank | BooleanRank | len   |
+--------+------------+-----------------+--------+-------------+-------+
|  16452 |      17637 | 942,2552,9084   |  32494 |          10 | 51790 |
|  16452 |      17827 | 31699           |  30232 |           3 | 51701 |
|  16452 |      17761 | 667,47646       |  29517 |           7 | 50264 |
|  16452 |      17791 | 13566           |  28888 |           3 | 49129 |
|  16452 |      17927 | 23259,7044      |  26731 |           7 | 48983 |
|  16452 |      17839 | 9012,199        |  24933 |           7 | 44451 |
|  16452 |      17815 | 29318           |  24745 |           3 | 44011 |
|  16452 |      17729 | 895,16485,24034 |  23305 |          10 | 42612 |
|  16452 |      17621 | 1765            |  19935 |           3 | 36698 |
|  16452 |      17803 | 3942            |  17552 |           3 | 30586 |
+--------+------------+-----------------+--------+-------------+-------+
10 rows in set (1,88 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The query expansion is useful when you intend to search which entries contain
more words that appear frequently within the search term. Having large text fields
increase the probability to have more words that appear among the search term.
In the case of &lt;code class=&quot;highlighter-rouge&quot;&gt;bookContent&lt;/code&gt; table (by paragraph table), the average field size
is &lt;code class=&quot;highlighter-rouge&quot;&gt;r rs$len&lt;/code&gt; characters.&lt;/p&gt;

&lt;h2 id=&quot;the-innodb_ft_index_table&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;INNODB_FT_INDEX_TABLE&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;There is a way to play with the contents of the FTS indexes. As you may noticed
in the previous examples, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;set global innodb_ft_aux_table = 'ftslab/bookContent';&lt;/code&gt;
statement, which loads the index content to memory for an easy querying.&lt;/p&gt;

&lt;p&gt;If you use RDS, the option &lt;code class=&quot;highlighter-rouge&quot;&gt;innodb_ft_aux_table&lt;/code&gt; is not available as it is GLOBAL
and require SUPER privileges.&lt;/p&gt;

&lt;p&gt;i.e. You can easily get the most frequent tokens:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;information_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INNODB_FT_INDEX_TABLE&lt;/span&gt;   
           &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;having&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;limit&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+----------+
| WORD   | count(*) |
+--------+----------+
| should |     1023 |
| yet    |     1027 |
| any    |     1070 |
| like   |     1071 |
| been   |     1073 |
| first  |     1080 |
| nor    |     1087 |
| your   |     1106 |
| thou   |     1130 |
| shall  |     1164 |
+--------+----------+
10 rows in set (5,40 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Probably it isn’t a very useful information as most of this words appears too frequently
and are modal verbs,  adverbs,  pronouns, determiners, etc. It could be the case that you
are not interested on indexing those words. If that’s the case you can add them as &lt;code class=&quot;highlighter-rouge&quot;&gt;stopwords&lt;/code&gt;
in your own stopwords table. Specially if you are more interested in boolean searches, loosing
some part of the language expressions. I built I query for this situation to allow us to build
the stopwords table using the current words that we want to add to the filtering:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(ftslab) &amp;gt; select group_concat(WORD) FROM (select distinct WORD  
  FROM information_schema.INNODB_FT_INDEX_TABLE               
  group by WORD having count(*) &amp;gt; 1000) d\G
*************************** 1. row ***************************
group_concat(WORD): all,and,any,been,but,can,first,had,has,have,her,him,his,into,
its,like,may,more,nor,not,now,one,only,other,our,said,shall,she,should,some,such,
than,their,them,then,there,these,they,those,thou,thus,thy,time,were,which,would,
yet,you,your
1 row in set (5,28 sec)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s build our filter table using both default and new entries and keeping the
alphabetical order:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine_stopwords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ENGINE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INNODB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;INSERT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INTO&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine_stopwords&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;INFORMATION_SCHEMA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INNODB_FT_DEFAULT_STOPWORD&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;UNION&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DISTINCT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;  
      &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;information_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INNODB_FT_INDEX_TABLE&lt;/span&gt;               
      &lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WORD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;having&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allEntries&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ASC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;DROP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ftscontent&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;innodb_ft_server_stopword_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ftslab/bookContentByLine_stopwords'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FULLTEXT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;INDEX&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ftscontent&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Checking the contents of the index is easy as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(ftslab) &amp;gt; select *
          from information_schema.INNODB_FT_INDEX_TABLE
          WHERE lower(WORD) like '%country%';
+------------------+--------------+-------------+-----------+--------+----------+
| WORD             | FIRST_DOC_ID | LAST_DOC_ID | DOC_COUNT | DOC_ID | POSITION |
+------------------+--------------+-------------+-----------+--------+----------+
| country          |          149 |         787 |        28 |    733 |      265 |
| country          |          149 |         787 |        28 |    733 |     1342 |
| countrydistricts |          733 |         733 |         1 |    733 |      816 |
| thecountry       |          249 |         733 |         2 |    733 |      750 |
+------------------+--------------+-------------+-----------+--------+----------+
4 rows in set (0,08 sec)

(ftslab) &amp;gt; select *
          from information_schema.INNODB_FT_INDEX_TABLE
          WHERE DOC_ID = 155 AND lower(WORD) like '%country%';
+---------+--------------+-------------+-----------+--------+----------+
| WORD    | FIRST_DOC_ID | LAST_DOC_ID | DOC_COUNT | DOC_ID | POSITION |
+---------+--------------+-------------+-----------+--------+----------+
| country |          149 |         787 |        28 |    155 |       31 |
| country |          149 |         787 |        28 |    155 |      495 |
| country |          149 |         787 |        28 |    155 |      158 |
| country |          149 |         787 |        28 |    155 |      525 |
+---------+--------------+-------------+-----------+--------+----------+
4 rows in set (0,09 sec)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the example shown before the is no intention to compare ranks score as they are based in different algorithms.
The idea there is to show that QUERY EXPANSION can have non desire results in some cases due to its mechanism.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;going-ahead-on-choosing-stop-words&quot;&gt;Going ahead on choosing stop words&lt;/h3&gt;

&lt;p&gt;The full &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf's_law&quot;&gt;article&lt;/a&gt; is amazingly interesting. In a brief, it says that
the most frequent word will occur approximately twice as often as the second most frequent word, three times as
often as the third most frequent word, and so on (rank-frequency distribution is an inverse relation).&lt;/p&gt;

&lt;p&gt;The idea here is to measure how much index do we safe cutting those words that are extremely frequent and don’t add
a necessary meaning to the search.&lt;/p&gt;

&lt;h2 id=&quot;considerations-and-recommendations&quot;&gt;Considerations and recommendations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use QUERY EXPANSION only if you are interested to search relations over exact matches. Remember that the field
size is crucial when using this.&lt;/li&gt;
  &lt;li&gt;FTS is not the best fit for exact string matches in single columns. You don’t want to use FTS for searching emails in
a single column, name and lastname fields , i.e. For those, you’ll probably use other techniques as reverse searches or
exact match operator (=).&lt;/li&gt;
  &lt;li&gt;Keep your FTS indexes short. Do not add ALL the text columns. Parse first from your application the user search and
adapt the query.&lt;/li&gt;
  &lt;li&gt;If you are using BOOLEAN MODE, you can use the rank score to filter rows. MySQL is clever enough to optimize the
FTS functions to avoid double executions. You can do this using something like:
&lt;code class=&quot;highlighter-rouge&quot;&gt;match(content,title) against (&quot;first (&amp;lt;second &amp;gt;third)&quot;) &amp;gt; 1 &lt;/code&gt;
Generally, scores lower than 1 can be ignored when using boolean or natural mode searches.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;OPTIMIZE TABLE&lt;/code&gt; does a rebuild of the table. To avoid this, set &lt;code class=&quot;highlighter-rouge&quot;&gt;innodb_optimize_fulltext_only=1&lt;/code&gt; in order to do an incremental
maintance on the table.&lt;/li&gt;
  &lt;li&gt;Recall that NATURAL LANGUAGE MODE does not take the operands as the BOOLEAN MODE. This affects the ranking score (try __+bad (&lt;feeling&gt;thing)__ i.e.)&lt;/feeling&gt;&lt;/li&gt;
  &lt;li&gt;If you plan to order by rank, it is not necessary to specify the clause &lt;code class=&quot;highlighter-rouge&quot;&gt;ORDER BY&lt;/code&gt; as InnoDB does the order after retrieve the doc ids . Also,the behavior is different from the default as it returns the heaviest at the top (like an &lt;strong&gt;ORDER BY rank DESC&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;If you come from MyISAM’s FTS implementation, recall that the ranking scoring is different.&lt;/li&gt;
  &lt;li&gt;Create the FULLTEXT index after the data is loaded &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/optimizing-innodb-bulk-data-loading.html&quot;&gt;InnoDB bulk load&lt;/a&gt;. When restoring FTS backups, you will probably hit the “ERROR 182 (HY000) at line nn: Invalid InnoDB FTS Doc ID”.&lt;/li&gt;
  &lt;li&gt;Try to avoid using use more than one FTS expression in the where clause. Keep in mind that this affects the order in the results and
it consumes a considerably amount of CPU. InnoDB orders by the latest expression in the WHERE clause. &lt;a href=&quot;https://dev.mysql.com/worklog/task/?id=7123&quot;&gt;WL#7123&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Also, if avoiding the rank information in the projection (SELECT clause) and using other aggregations like &lt;code class=&quot;highlighter-rouge&quot;&gt;count(*)&lt;/code&gt;, will use
the “no ranking” FT_hints. The &lt;code class=&quot;highlighter-rouge&quot;&gt;limit&lt;/code&gt; hint won’t be used if invoked explicitely an &lt;code class=&quot;highlighter-rouge&quot;&gt;ORDER BY&lt;/code&gt; and th &lt;code class=&quot;highlighter-rouge&quot;&gt;MATCH&lt;/code&gt; clause in the projection.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;explain&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;against&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;+home&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BOOLEAN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt;  
                      &lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;G&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;select_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SIMPLE&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
         &lt;span class=&quot;k&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fulltext&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Extra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Using&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ft_hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no_ranking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filesort&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;explain&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;against&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;+home&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BOOLEAN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                      &lt;span class=&quot;k&quot;&gt;LIMIT&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;G&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
         &lt;span class=&quot;k&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fulltext&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Extra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Using&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ft_hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no_ranking&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;limit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;explain&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;against&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;+home&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BOOLEAN&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;G&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bookContentByLine&lt;/span&gt;
         &lt;span class=&quot;k&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fulltext&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Extra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Using&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ft_hints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no_ranking&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;If you plan to use &lt;code class=&quot;highlighter-rouge&quot;&gt;FTS_DOC_ID&lt;/code&gt; column with &lt;code class=&quot;highlighter-rouge&quot;&gt;AUTO_INCREMENT&lt;/code&gt; option,
have in mind that there is a limitation regarding this. You must declare
a single column PRIMARY KEY constraint or as an UNIQUE index. Also, the data
type is stricted as &lt;code class=&quot;highlighter-rouge&quot;&gt;bigint unsigned&lt;/code&gt;. i.e:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;AUTO_INCREMENT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mainPk&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bigint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;PRIMARY&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mainPk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;UNIQUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FTS_DOC_ID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ft_query_expansion_limit&quot;&gt;FT_QUERY_EXPANSION_LIMIT&lt;/h3&gt;

&lt;p&gt;This variable controls the number of top matches when using &lt;code class=&quot;highlighter-rouge&quot;&gt;WITH QUERY EXPANSION&lt;/code&gt; (affects only MyISAM).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_ft_query_expansion_limit&quot;&gt;reference&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;bug-80347---invalid-innodb-fts-doc-id&quot;&gt;Bug 80347 - Invalid InnoDB FTS Doc ID&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Submitted https://bugs.mysql.com/bug.php?id=80347&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;emanuel@3laptop ~/sandboxes/rsandbox_5_7_9 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./m dumpTest &amp;lt; full.dump
ERROR 182 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HY000&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; at line 73: Invalid InnoDB FTS Doc ID

emanuel@3laptop ~/sandboxes/rsandbox_5_7_9 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./m dumpTest &amp;lt; ddl.dump
emanuel@3laptop ~/sandboxes/rsandbox_5_7_9 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./m dumpTest &amp;lt; onlyData.dump
emanuel@3laptop ~/sandboxes/rsandbox_5_7_9 &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./m dumpTest &amp;lt; full.dump
ERROR 182 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HY000&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; at line 73: Invalid InnoDB FTS Doc ID
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;mysqldump is not very clever if you use &lt;code class=&quot;highlighter-rouge&quot;&gt;FTS_DOC_ID&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2016-02-13T22:11:53.125300Z 19 [ERROR] InnoDB: Doc ID 10002 is too big. Its difference with largest used Doc ID 1 cannot exceed or equal to 10000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It takes dumps without considering the restriction coded in
&lt;code class=&quot;highlighter-rouge&quot;&gt;innobase/row/row0mysql.cc&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Difference between Doc IDs are restricted within
4 bytes integer. See fts_get_encoded_len()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The fix to this is backuping the table by chunks of 10000 documents.&lt;/p&gt;

&lt;h3 id=&quot;fine-tuning&quot;&gt;Fine tuning&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/fulltext-fine-tuning.html&quot;&gt;Fine tuning&lt;/a&gt;
&lt;a href=&quot;https://blogs.oracle.com/mysqlinnodb/entry/innodb_full_text_search_performance&quot;&gt;Performance&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;features-introduces&quot;&gt;Features introduces&lt;/h3&gt;

&lt;h3 id=&quot;maintenance&quot;&gt;Maintenance&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_optimize_fulltext_only&quot;&gt;innodb_optimize_fulltext_only&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;parsers-internals&quot;&gt;Parsers internals&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/writing-full-text-plugins.html&quot;&gt;Writting FTS parser plugins&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;version&quot;&gt;Version&lt;/h2&gt;

&lt;p&gt;This R Markdown document is made interactive using Shiny. Unlike the more traditional
workflow of creating static reports, you can now create documents that allow your
readers to change the assumptions underlying your analysis and see the results immediately.&lt;br /&gt;
To learn more, see &lt;a href=&quot;http://rmarkdown.rstudio.com/authoring_shiny.html&quot;&gt;Interactive Documents&lt;/a&gt;.&lt;/p&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/fts-innodb</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/FTS-in-MySQL-5-7-with-InnoDB</guid>
                <pubDate>Wed, 20 Apr 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Multi source data injection to Postgres RDS with encryption and FTS support</title>
                <description>&lt;p&gt;Sponsored: &lt;a href=&quot;http://pythian.com&quot;&gt;Pythian Inc.&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 1:
All the of this presentation is published in this &lt;a href=&quot;&quot;&gt;repository&lt;/a&gt;. You will find a lot of folders and information, probably part of a blog series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 2:
All the work on this article is a &lt;strong&gt;POC&lt;/strong&gt; (Proof of concept).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note 3:
This is something that is related for &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;&gt;HIPAA&lt;/a&gt; compliant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;kmsrds&quot;&gt;&lt;a href=&quot;http://aws.amazon.com/kms/&quot;&gt;KMS&lt;/a&gt;/&lt;a href=&quot;https://aws.amazon.com/rds/postgresql/&quot;&gt;RDS&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The POC on this article was developed before the releasing of the Key Management service
for RDS.&lt;/p&gt;

&lt;p&gt;I totally discourage to use the current approach for encrypting data. &lt;em&gt;Use REST.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I’ve been dealing with an issue that came into my desktop from people of the
community, regarding RDS and HIPAA rules. There was a confusing scenario whether
PostgreSQL was using FTS and encryption on RDS. There are a lot of details regarding
the architecture, however I think it won’t be necessary to dig into
very deeply to understand the basics of the present article moto.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;&gt;HIPAA&lt;/a&gt;
rules are complex and if you need to deal with them, you’ll probably need to go
through a careful read.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tl;dr&lt;/code&gt;? they tell us to store data encrypted on servers that are not in the premises.
And that’s the case of RDS. However, all the communications are encrypted using
SSL protocol, but is not enough to compliant with HIPAA rules.&lt;/p&gt;

&lt;p&gt;CPU resources in RDS are expensive and not stable sometimes, which makes encryption and
FTS features not very well suited for this kind of service. I not saying that you
can’t implement them, just keep in mind that a standard CPU against vCPU could
have a lot difference. If you want to benchmark your local CPU against RDS vCPU,
you can run the following query inside &lt;code class=&quot;highlighter-rouge&quot;&gt;psql&lt;/code&gt; on both instances:&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;null&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timing&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pgp_sym_decrypt_bytea&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;pgp_sym_encrypt_bytea&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Text to be encrypted using pgp_sym_decrypt_bytea'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gen_random_uuid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bytea&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'compress-algo=2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
          &lt;span class=&quot;s1&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;s1&quot;&gt;'SQL-ASCII'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There are a lot of things and functions you can combine from the &lt;code class=&quot;highlighter-rouge&quot;&gt;pgcrypto&lt;/code&gt; package
(you will see that the repository contemplates all of them).
I will try to post another blog post regarding this kind of benchmarks. In the
meantime, this query should be enough to have a rough idea of the performance difference
between RDS instance vCPU and your server CPUs.&lt;/p&gt;

&lt;h2 id=&quot;architecture-basics&quot;&gt;Architecture basics&lt;/h2&gt;

&lt;p&gt;For this POC we are going to store FTS and GPG keys locally, in a simple PostgreSQL
instance and, using a trigger, encrypt and upload transparently to RDS using the
standard FDW (Foreign Data Wrappers).&lt;/p&gt;

&lt;p&gt;Have in mind that RDS communication is already encrypted via SSL when data flows
between server/client. It’s important to clarify this, to avoid confusions between
communication encryption and storing data encrypted.&lt;/p&gt;

&lt;p&gt;The simple trigger will split the unencrypted data between a local table storing
in a &lt;code class=&quot;highlighter-rouge&quot;&gt;tsvector&lt;/code&gt; column (jsonb in the TODO), it will encrypt and push the encrypted
data into RDS using FDW (the standard postgres_fdw package).&lt;/p&gt;

&lt;p&gt;A simple flight view of the idea can be observed in the image bellow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/3manuek/RDS_HIPAA_FTS/master/FDW_TO_RDS/images/image1.png&quot; alt=&quot;alt text&quot; title=&quot;Image 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Source: https://www.lucidchart.com/documents/edit/c22ce7a1-c09d-4ca8-922d-dcb123d577a5?driveId=0AHk8my7IafcZUk9PVA#&lt;/p&gt;

&lt;h2 id=&quot;rds-structure-and-mirrored-local-structure-with-fdw&quot;&gt;RDS structure and mirrored local structure with FDW&lt;/h2&gt;

&lt;p&gt;RDS instance schema structure contains a parent table , a partitioning trigger  and
its trigger:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE SCHEMA enc_schema;

SET search_path TO enc_schema;

-- Encrpting locally, that's why we don't need to reference the key here.
create table enc_schema.__person__pgp
     (
      id bigint,
      source varchar(8),
      partial_ssn varchar(4), -- Non encrypted field for other fast search purposes
      ssn bytea,
      keyid varchar(16),
      fname bytea,
      lname bytea,
      description bytea,
      auth_drugs bytea, 		-- This is an encrypted text vector
      patology bytea,
      PRIMARY KEY(id,source)
);

CREATE INDEX ON enc_schema.__person__pgp (partial_ssn);


CREATE OR REPLACE FUNCTION basic_ins_trig() RETURNS trigger LANGUAGE plpgsql AS $basic_ins_trig$
DECLARE
  compTable text :=  TG_RELID::regclass::text ;
  childTable text := compTable || '_' || NEW.source ;
  statement text :=  'INSERT INTO ' || childTable || ' SELECT (' || QUOTE_LITERAL(NEW) || '::'  || compTable ||  ').*' ;
  createStmt text := 'CREATE TABLE ' || childTable  ||
    '(CHECK (source =' || quote_literal(NEW.source) || ')) INHERITS (' || compTable || ')';
  indexAdd1 text := 'CREATE INDEX ON ' || childTable || '(source,id)' ;
  indexAdd2 text := 'CREATE INDEX ON ' || childTable || '(source,ssn)' ;
BEGIN
  BEGIN
    EXECUTE statement;
  EXCEPTION
    WHEN undefined_table THEN
      EXECUTE createStmt;
      EXECUTE indexAdd1;
      EXECUTE indexAdd2;
      EXECUTE statement;
  END;
  RETURN NULL;

END;

$basic_ins_trig$;


CREATE TRIGGER part_person_pgp BEFORE INSERT ON __person__pgp
FOR EACH ROW EXECUTE PROCEDURE basic_ins_trig() ;


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We are not going to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;partial SSN&lt;/code&gt; column in the examples, but I found it very helpful to
do RDS searches over encrypted data without fall into the need of decrypting in-the-fly the SSN.
The last SSN’s 4-digits do not provide useful information if stolen.&lt;/p&gt;

&lt;p&gt;Also, the magic of the multi-source data injection comes from the compound key using a
bigint and a source tag.&lt;/p&gt;

&lt;p&gt;Basically, you can think on the local nodes as proxies. You can insert data on every node,
but the data will point to the RDS instance.&lt;/p&gt;

&lt;p&gt;If you are planning to manage large amounts of data, you can partition the table on RDS,
allowing a better organization for data management.&lt;/p&gt;

&lt;p&gt;You will see no indexes over encrypted data&lt;/p&gt;

&lt;p&gt;Local nodes structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE DATABASE fts_proxy;  --  connect using \c fts_proxy on psql

-- The sauce
CREATE EXTENSION postgres_fdw;
CREATE EXTENSION pgcrypto;

CREATE SERVER RDS_server
        FOREIGN DATA WRAPPER postgres_fdw
        OPTIONS (host 'dbtest1.chuxsnuhtvgl.us-east-1.rds.amazonaws.com', port '5432', dbname 'dbtest');

CREATE USER MAPPING FOR postgres
        SERVER RDS_server
        OPTIONS (user 'dbtestuser', password '&amp;lt;shadowed&amp;gt;');

CREATE FOREIGN TABLE __person__pgp_RDS
(
       id bigint,
       source varchar(8),
       partial_ssn varchar(4), -- Non encrypted field for other fast search purposes
       ssn bytea,
       keyid varchar(16),
       fname bytea,
       lname bytea,
       description bytea,
       auth_drugs bytea, -- This is an encrypted text vector
       patology bytea
)
SERVER RDS_server
OPTIONS (schema_name 'enc_schema', table_name '__person__pgp');
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Same table. Everytime we want to deal with the RDS table, we are going to do so using the &lt;code class=&quot;highlighter-rouge&quot;&gt;__person__pgp_RDS&lt;/code&gt; table, which is just a mapping table. We can query this table as any other usual table.&lt;/p&gt;

&lt;p&gt;For testing purposes, I also created a table with the same structure as the above with
&lt;code class=&quot;highlighter-rouge&quot;&gt;__person_rds_RDS_URE&lt;/code&gt; table name and added the &lt;code class=&quot;highlighter-rouge&quot;&gt;use_remote_estimate 'true'&lt;/code&gt; option.
When enabled, postgres_fdw obtains the row count and estimates from the remote server.&lt;/p&gt;

&lt;h2 id=&quot;inserting-keys-locally&quot;&gt;Inserting keys locally&lt;/h2&gt;

&lt;p&gt;Just to avoid an extended article, I will skip the GPG key creation commands here. Please follow the instructions on the link at the referece section about keys.&lt;/p&gt;

&lt;p&gt;We can insert they keys in several ways, but I found very convenient to use &lt;code class=&quot;highlighter-rouge&quot;&gt;psql&lt;/code&gt;
features to do so. Once the keys are in place you can use &lt;code class=&quot;highlighter-rouge&quot;&gt;\lo_import&lt;/code&gt; command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;postgres=# \lo_import /var/lib/postgresql/9.4/main/private.key
lo_import 33583
postgres=# \lo_import /var/lib/postgresql/9.4/main/public.key
lo_import 33584
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The next steps are very straightforward. In a real scenario, you won’t probably
want to upload private keys into the table, just for practical purposes of this
article I’m going to do so (only for decrypt data in the SELECT query).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pgp_key_id&lt;/code&gt; will return the same key no matter if you use private or public key.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE keys (
   keyid varchar(16) PRIMARY KEY,
   pub bytea,
   priv bytea
);

INSERT INTO keys VALUES ( pgp_key_id(lo_get(33583)) ,lo_get(33584), lo_get(33583));
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;splitting-data-to-fts-encrypt-and-push-into-rds&quot;&gt;Splitting data to FTS, encrypt and push into RDS&lt;/h2&gt;

&lt;p&gt;Now, here is when the tricky part starts. We are going to achieve some functionalities:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We are going to simulate &lt;em&gt;routing&lt;/em&gt; using inheritance on the FTS records. That will allow us to split data as we want and, replicate using Logical Decoding feature between the nodes. I won’t include this on the current article just to avoid it to be extense.&lt;/li&gt;
  &lt;li&gt;We are going to encrypt using the key that we select on the insert query. If you want a key &lt;em&gt;per table basis&lt;/em&gt;, you will find easier to hardcode the key id on the &lt;code class=&quot;highlighter-rouge&quot;&gt;_func_get_FTS_encrypt_and_push_to_RDS&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Once the records are encrypted, the function will insert those records to the foreign table (RDS).&lt;/li&gt;
  &lt;li&gt;When querying the FTS table, we will be able to determine the source (something like the &lt;code class=&quot;highlighter-rouge&quot;&gt;routing&lt;/code&gt; technique, you will find this familiar if you played with ElasticSearch). That allow us to make the FTS search transparent to the application, pointing always to the parent table. :dogewow:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Isn’t Postgres cool? :o&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;fts-table-structures&quot;&gt;FTS table structures&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-- Parent table
CREATE TABLE local_search (
  id bigint PRIMARY KEY,
  _FTS tsvector
);
CREATE INDEX fts_index ON local_search USING GIST(_FTS);

-- Child table, suffix local_search_&amp;lt;source&amp;gt;

CREATE TABLE local_search_host1 () INHERITS (local_search);
CREATE INDEX fts_index_host1 ON local_search_host1 USING GIST(_FTS);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Doing this, you avoid to have a column with a constant value in the table, consuming unnecessary space. You can have with this method, different names and tables accross the cluster, but always using the same query against &lt;code class=&quot;highlighter-rouge&quot;&gt;local_search&lt;/code&gt;. You can map/reduce the data if you want to across the nodes, with the very same query.&lt;/p&gt;

&lt;p&gt;Is not necessary to only have 1 source or route per node. The only requirement for this is to have different routes per node (combining source and route could increase complexity, however is possible).&lt;/p&gt;

&lt;h2 id=&quot;main-code&quot;&gt;Main code&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE SEQUENCE global_seq INCREMENT BY 1 MINVALUE 1 NO MAXVALUE;


CREATE TABLE __person__pgp_map
     (
      keyid varchar(16),
      source varchar(8),
      ssn bigint,
      fname text,
      lname text,
      description text,
      auth_drugs text[], -- This is an encrypted text vector
      patology text
    );

CREATE OR REPLACE FUNCTION _func_get_FTS_encrypt_and_push_to_RDS() RETURNS &quot;trigger&quot; AS $$
DECLARE
        secret bytea;
        RDS_MAP __person__pgp_RDS%ROWTYPE;
        FTS_MAP local_search%ROWTYPE;
BEGIN

    SELECT pub INTO secret FROM keys WHERE keyid = NEW.keyid;

    RDS_MAP.source := NEW.source;
    RDS_MAP.fname := pgp_pub_encrypt(NEW.fname, secret);
    RDS_MAP.lname := pgp_pub_encrypt(NEW.lname, secret);
    RDS_MAP.auth_drugs := pgp_pub_encrypt(NEW.auth_drugs::text, secret);
    RDS_MAP.description := pgp_pub_encrypt(NEW.description, secret);
    RDS_MAP.patology := pgp_pub_encrypt(NEW.patology, secret);
    RDS_MAP.ssn := pgp_pub_encrypt(NEW.ssn::text, secret);
    RDS_MAP.partial_ssn := right( (NEW.ssn)::text,4);
    RDS_MAP.id := nextval('global_seq'::regclass);

    RDS_MAP.keyid := NEW.keyid;

    FTS_MAP.id   := RDS_MAP.id;
    FTS_MAP._FTS := (setweight(to_tsvector(NEW.fname) , 'B' ) ||
                   setweight(to_tsvector(NEW.lname), 'A') ||
                   setweight(to_tsvector(NEW.description), 'C') ||
                   setweight(to_tsvector(NEW.auth_drugs::text), 'C') ||
                   setweight(to_tsvector(NEW.patology), 'D')
                    ) ;

    -- Both tables contain same id,source
    INSERT INTO __person__pgp_RDS SELECT (RDS_MAP.*);
    EXECUTE 'INSERT INTO local_search_' || NEW.source || ' SELECT (' ||  quote_literal(FTS_MAP) || '::local_search).* ';
   RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER trigger_befInsRow_name_FTS
BEFORE INSERT ON __person__pgp_map
FOR EACH ROW
EXECUTE PROCEDURE _func_get_FTS_encrypt_and_push_to_RDS();
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This functions does everything. It inserts the data on RDS and split the data on the corresponding FTS child table. For performance purposes, I didn’t want to catch exceptions at insert time (if the child table does not exists, i.e.), but you can also add this feature with an exception block as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   BEGIN
    EXECUTE 'INSERT INTO local_search_' || NEW.source || ' SELECT (' ||  quote_literal(FTS_MAP) || '::local_search).* ';
   EXCEPTION WHEN undefined_table THEN
     EXECUTE 'CREATE TABLE local_search_' || NEW.source || '() INHERITS local_search';
   END;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The same can be done over the foreign table. More info in “Class HV — Foreign Data Wrapper Error (SQL/MED)” (HV00R -&lt;code class=&quot;highlighter-rouge&quot;&gt;fdw_table_not_found&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Check “Appendix A. PostgreSQL Error Codes” on the official manual for references about error codes.&lt;/p&gt;

&lt;h3 id=&quot;inserting-data&quot;&gt;Inserting data&lt;/h3&gt;

&lt;p&gt;At insertion time, we are going to push data through a mapping table. The reason for this is that all the encrypted data is stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;bytea&lt;/code&gt; datatype, and we want to have clear queries instead.&lt;/p&gt;

&lt;p&gt;A random data query will look as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INSERT INTO __person__pgp_map
  SELECT
      'host1',  -- source: host1
                -- You can do this better by grabbing this data from a persistent
                -- location
      '76CDA76B5C1EA9AB',
       round(random()*1000000000),
      ('{Romulo,Ricardo,Romina,Fabricio,Francisca,Noa,Laura,Priscila,Tiziana,Ana,Horacio,Tim,Mario}'::text[])[round(random()*12+1)],
      ('{Perez,Ortigoza,Tucci,Smith,Fernandez,Samuel,Veloso,Guevara,Calvo,Cantina,Casas,Korn,Rodriguez,Ike,Baldo,Vespi}'::text[])[round(random()*15+1)],
      ('{some,random,text,goes,here}'::text[])[round(random()*5+1)] ,
      get_drugs_random(round(random()*10)::int),
      ('{Anotia,Appendicitis,Apraxia,Argyria,Arthritis,Asthma,Astigmatism,Atherosclerosis,Athetosis,Atrophy,Abscess,Influenza,Melanoma}'::text[])[round(random()*12+1)]
      FROM generate_series(1,50) ;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Did you see the inner comment? Well, probably you want to split by &lt;code class=&quot;highlighter-rouge&quot;&gt;customer&lt;/code&gt; or any other alias. I’m using this ugly harcoded text just to avoid a long article.&lt;/p&gt;

&lt;p&gt;Also, if you want to avoid harcoding as much as posible, you can consider to use a function that returns the host name or routing tag.&lt;/p&gt;

&lt;h3 id=&quot;querying-the-data&quot;&gt;Querying the data&lt;/h3&gt;

&lt;p&gt;We are almost done! Now we can do  some queries. Here are some examples:&lt;/p&gt;

&lt;p&gt;Limiting the matches:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# SELECT convert_from(pgp_pub_decrypt(ssn::text::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name)
# FROM __person__pgp_rds as rds JOIN
#       keys ks USING (keyid)
# WHERE rds.id IN (
#                select id
#                from local_search
#                where to_tsquery('Asthma | Athetosis') @@ _fts LIMIT 5)
#   AND rds.source = 'host1';

 source | convert_from
--------+--------------
 host1  | 563588056
(1 row)               

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;All the matches and double check from were the data came from:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# SELECT ls.tableoid::regclass, rds.source,
#        convert_from(pgp_pub_decrypt(ssn::text::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name)
# FROM local_search ls JOIN
#     __person__pgp_rds as rds USING (id),
#     keys ks
# WHERE to_tsquery('Asthma | Athetosis') @@ ls._fts;

     tableoid      | source | convert_from
-------------------+--------+--------------
local_search_host1 | host1  | 563588056
(1 row)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And, we can’t finish the article without showing how to use the ranking (did you see those setweight
functions used in the function? You got it!):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#  SELECT rds.id,
#  convert_from(pgp_pub_decrypt(fname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
#  convert_from(pgp_pub_decrypt(lname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
#  ts_rank( ls._FTS, query ) as rank
#    FROM local_search ls JOIN
#         __person__pgp_rds as rds ON (rds.id = ls.id AND rds.source = 'host1') JOIN
#         keys ks USING (keyid),
#         to_tsquery('Mario | Casas | (Casas:*A &amp;amp; Mario:*B) ') query
#    WHERE
#        ls._FTS  @@ query
#    ORDER BY rank DESC;

 id | convert_from | convert_from |   rank   
----+--------------+--------------+----------
 43 | Mario        | Casas        | 0.425549
 61 | Ana          | Casas        | 0.303964
 66 | Horacio      | Casas        | 0.303964
(3 rows)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Remember, think that this query is doing FTS, decryption and ranking in just one query, over a local and
a remote server. You can’t say that PostgreSQL isn’t hipster enough!&lt;/p&gt;

&lt;p&gt;I can’t continue the article without showing the query plan executed by the local host (using buffers,
  analyze and verbose options).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EXPLAIN (buffers,verbose,analyze) SELECT rds.id,
 convert_from(pgp_pub_decrypt(fname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
 convert_from(pgp_pub_decrypt(lname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
 ts_rank( ls._FTS, query ) as rank
   FROM local_search ls JOIN
        __person__pgp_rds as rds ON (rds.id = ls.id AND rds.source = 'host1') JOIN
        keys ks USING (keyid),
        to_tsquery('Mario | Casas | (Casas:*A &amp;amp; Mario:*B) ') query
   WHERE
       ls._FTS  @@ query
   ORDER BY rank DESC;


....
               -&amp;gt;  Materialize  (cost=100.00..117.09 rows=3 width=122) (actual time=62.946..62.971 rows=50 loops=9)
                     Output: rds.id, rds.fname, rds.lname, rds.keyid
                     -&amp;gt;  Foreign Scan on public.__person__pgp_rds rds  (cost=100.00..117.07 rows=3 width=122) (actual time=566.495..566.520 rows=50 loops=1)
                           Output: rds.id, rds.fname, rds.lname, rds.keyid
                           Remote SQL: SELECT id, keyid, fname, lname FROM enc_schema.__person__pgp WHERE ((source = 'host1'::text))
...
 Planning time: 4.931 ms
 Execution time: 2115.919 ms
(45 rows)

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;From the above &lt;em&gt;Query Plan&lt;/em&gt; extract, we can see that the partitioning at RDS is transparent for the query.
The execution node in charge of extracting data from the RDS is the &lt;em&gt;Foreign Scan&lt;/em&gt;, which also
provides the query executed remotely.&lt;/p&gt;

&lt;p&gt;Wait a minute. Looks like the remote SQL is somehow dangerous to execute. It is not using the
&lt;em&gt;id&lt;/em&gt;! There is a reason for that, and its related on how postgres gather the foreign table statistics.
If I use the &lt;em&gt;remote estimations&lt;/em&gt; we can see how the remote SQL changes in the Query Plan:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; EXPLAIN (ANALYZE, VERBOSE, BUFFERS) SELECT rds.id,
      convert_from(pgp_pub_decrypt(fname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      convert_from(pgp_pub_decrypt(lname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      ts_rank( ls._FTS, query ) as rank
        FROM local_search ls,  __person__pgp_rds_URE  rds  JOIN
             keys ks USING (keyid),
             to_tsquery('Mario | Casas | (Casas:*A &amp;amp; Mario:*B) ') query
        WHERE                                                      
            rds.id = ls.id
              AND rds.source = 'host1'
            AND
            ls._FTS  @@ query
        ORDER BY rank DESC;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Query Plan (Foreign Scan execution node):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...
-&amp;gt;  Foreign Scan on public.__person__pgp_rds_ure rds  (cost=100.01..108.21 rows=2 width=1018) (actual time=250.334..250.336 rows=1 loops=31)
      Output: rds.id, rds.source, rds.partial_ssn, rds.ssn, rds.keyid, rds.fname, rds.lname, rds.description, rds.auth_drugs, rds.patology
      Remote SQL: SELECT id, keyid, fname, lname FROM enc_schema.__person__pgp WHERE ((source = 'host1'::text)) AND (($1::bigint = id))
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Foreign tables also need the local statistics to be updated. In the next examples
there are 3 queries: using the &lt;code class=&quot;highlighter-rouge&quot;&gt;use_remote_estimate&lt;/code&gt;, without previous ANALYZE and
without &lt;code class=&quot;highlighter-rouge&quot;&gt;use_remote_estimate&lt;/code&gt; and a query using the local estimations (&lt;code class=&quot;highlighter-rouge&quot;&gt;__person_pgp_rds&lt;/code&gt;)
after issuing ANALYZE and without &lt;em&gt;URE&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fts_proxy=# \o /dev/null
fts_proxy=#  SELECT rds.id,
      convert_from(pgp_pub_decrypt(fname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      convert_from(pgp_pub_decrypt(lname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      ts_rank( ls._FTS, query ) as rank
        FROM local_search ls,  __person__pgp_rds_URE  rds  JOIN
             keys ks USING (keyid),
             to_tsquery('Mario | Casas | (Casas:*A &amp;amp; Mario:*B) ') query
        WHERE
            rds.id = ls.id
              AND rds.source = 'host1'
            AND
            ls._FTS  @@ query
        ORDER BY rank DESC;
Time: 12299,691 ms

fts_proxy=#  SELECT rds.id,
      convert_from(pgp_pub_decrypt(fname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      convert_from(pgp_pub_decrypt(lname::bytea, ks.priv,''::text)::bytea,'SQL_ASCII'::name),
      ts_rank( ls._FTS, query ) as rank
        FROM local_search ls,  __person__pgp_rds  rds  JOIN
             keys ks USING (keyid),
             to_tsquery('Mario | Casas | (Casas:*A &amp;amp; Mario:*B) ') query
        WHERE
            rds.id = ls.id
              AND rds.source = 'host1'
            AND
            ls._FTS  @@ query
        ORDER BY rank DESC;
Time: 20249,719 ms

-- AFTER ANALYZE on the FOREIGN TABLE __person_pgp_rds (in the local server)

Time: 1656,912 ms

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After analyzing both foreign tables , the execution time difference was calculated
at 11% in favor of using local estimations.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NOTE about UPDATES: it is necessary to code the UPDATE trigger also, in order to
decrypt , modify and re-encrypt the data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;jsonjsonb-datatype-is-here-to-help&quot;&gt;Json/jsonb datatype is here to help&lt;/h3&gt;

&lt;p&gt;You can collapse all the data and use &lt;code class=&quot;highlighter-rouge&quot;&gt;json&lt;/code&gt; datatype on the mapping and foreign table, allowing you to avoid the pain of pointing and decrypting data per column basis.&lt;/p&gt;

&lt;p&gt;Put all the encrypted columns in a &lt;code class=&quot;highlighter-rouge&quot;&gt;bytea&lt;/code&gt; column on RDS. The mapping table will look as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE __person__pgp_map
     (
      keyid varchar(16),
      source varchar(8),
      ssn bigint,
      data jsonb
    );
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At insert time, just use a json column instead per column basis. Keep in mind that you will need to deal within the json contents.
I found using this easier for insert, but the FTS needs some clean up to avoid insert column names in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_fts&lt;/code&gt; field at &lt;code class=&quot;highlighter-rouge&quot;&gt;local_search&lt;/code&gt; tables.
Also, for updates, the jsonb datatype will need extra work when extracting attributes.&lt;/p&gt;

&lt;h2 id=&quot;additional-functions-used-here&quot;&gt;Additional functions used here&lt;/h2&gt;

&lt;p&gt;In the insert statement above, you will see a user defined function that gets a random length vector of drugs. It is implemented using the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE drugsList ( id serial PRIMARY KEY, drugName text);

INSERT INTO drugsList(drugName) SELECT p.nameD FROM regexp_split_to_table(
'Acetaminophen
Adderall
Alprazolam
Amitriptyline
Amlodipine
Amoxicillin
Ativan
Atorvastatin
Azithromycin
Ciprofloxacin
Citalopram
Clindamycin
Clonazepam
Codeine
Cyclobenzaprine
Cymbalta
Doxycycline
Gabapentin
Hydrochlorothiazide
Ibuprofen
Lexapro
Lisinopril
Loratadine
Lorazepam
Losartan
Lyrica
Meloxicam
Metformin
Metoprolol
Naproxen
Omeprazole
Oxycodone
Pantoprazole
Prednisone
Tramadol
Trazodone
Viagra
Wellbutrin
Xanax
Zoloft', '\n') p(nameD);

CREATE OR REPLACE FUNCTION get_drugs_random(int)
       RETURNS text[] AS
      $BODY$
      WITH rdrugs(dname) AS (
        SELECT drugName FROM drugsList p ORDER BY random() LIMIT $1
      )
      SELECT array_agg(dname) FROM rdrugs ;
$BODY$
LANGUAGE 'sql' VOLATILE;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;A very awesome tutorial about FTS for PostgreSQL can be found &lt;a href=&quot;http://www.sai.msu.su/~megera/postgres/fts/doc/appendixes.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.drugs.com/drug_information.html&quot;&gt;Source for drugs list&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simple.wikipedia.org/wiki/List_of_diseases&quot;&gt;Source for diseases&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.gnupg.org/gph/en/manual/c14.html&quot;&gt;Getting started with GPG keys&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS command line tool&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Discussion in the community mailing lis &lt;a href=&quot;http://postgresql.nabble.com/Fast-Search-on-Encrypted-Feild-td1863960.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/rds-hipaa-fts</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/Multi-source-data-injection-to-Postgres-RDS-with-encryption-and-local-FTS-support</guid>
                <pubDate>Wed, 23 Mar 2016 00:00:00 -0300</pubDate>
        </item>

        <item>
                <title>Ser anónimo, el derecho del futuro.</title>
                <description>&lt;blockquote&gt;
  &lt;p&gt;Agradecimiento a Guido Vilariño por la revisión del artículo.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hace unos días me topé con un artículo acerca del &lt;a href=&quot;http://www.avclub.com/article/facebook-tinkered-users-feeds-massive-psychology-e-206324&quot;&gt;Experimental evidence of massive-scale emotional contagion through social networks&lt;/a&gt; (Evidencia experimental del contagio emocional a gran escala a través de las redes sociales) llevado a cabo por científicos de Facebook. Este programa manipuló el contenido visualizado por 600,000 usuarios de esta red social, a fin de determinar como se influye en el estado de ánimo, utilizando las publicaciones posteriores para el análisis. Si deseas leer el paper completo, tal como se publicó en la Academia Nacional de Ciencias de USA, puedes verlo &lt;a href=&quot;http://www.pnas.org/content/111/24/8788.full.pdf?sid=d23bc81f-4f3c-4abb-9d0c-ed7416577b2a&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Se hace evidente, que alguien al que le preocupe en cierto grado la privacidad, este artículo es como leer IT de Stephen King a los 10 años.&lt;/p&gt;

&lt;p&gt;Este experimento -aplicable a cualquier red social-, podría ser el futuro de la propaganda política. Tan efectivo como la televisión, mucho más métrico y rápido. Este constante “feedback”, permite modificar las reglas en plena aplicación e incluso desviar el objetivo final, si hay otros intereses en juego. La manipulación de los medios con un fin negativo, está a la altura de ser tan repudiada como la censura.&lt;/p&gt;

&lt;p&gt;Piensa que tu vida, indefectiblemente se verá afectada. Por más que no uses redes sociales, la gente que te rodea habrá sido influenciada. ¿No sigues la corriente? Pues, estás fuera. Imagínate esta herramienta en manos de gobiernos populistas o dictatoriales. Me tiembla la espinilla. Incluso, podría crear ídolos o talentos falsos, políticos con doble cara (&lt;em&gt;ejem…&lt;/em&gt;) o incluso recobrar modas del pasado (xenofobia, intolerancia a la homosexualidad, etc).&lt;/p&gt;

&lt;p&gt;Ya no será necesaria una costumbre para hacerla ley. Ya podrían influir, para crear una costumbre y luego hacerla ley.&lt;/p&gt;

&lt;p&gt;Hasta ahora no hay nada nuevo. ¡Claro que ya lo sabías, campeón! Eso es bueno, porque la principal herramienta de combatirlo es justamente eso: sabiéndolo. Es un buen paso, aunque no el último. Quizás, el siguiente sea no ser fatalista.&lt;/p&gt;

&lt;p&gt;De ahí que radique la importancia de educar a las próximas generaciones, con respecto a como y cuando permanecer anónimos. La práctica de este derecho podría llegar a ser la máxima representación de la libre expresión. Esta libertad por si sola, no garantiza la protección contra las represalias. Sin embargo, el anonimato, es una forma colaborativa de recepción y emisión de información.&lt;/p&gt;

&lt;p&gt;Para mantener la libertad de expresión en un estado saludable, hay que entrenar en las próximas generaciones el sentido común y la objetividad.&lt;/p&gt;

&lt;p&gt;El aprender a ser anónimos ya no por miedo, sino por derecho, nos permite compartir y recibir información sin prejuicios acerca de quienes fuimos hasta ese momento.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Aprende a ser anónimo, por que es el derecho del futuro.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;¿Que cosas tengo que tener en cuenta?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Internet es una herramienta, que intenta ser libre -en principio-. Esa libertad trae aparejado tanto lo bueno como lo malo. No todo es cierto, no todo es falso. Hay muchos gatos, de todo tipo.&lt;/li&gt;
  &lt;li&gt;Buscar en internet es uno de los tantos verbos que esta permite. Y en la mayor parte
de las búsquedas no son anónimas.&lt;/li&gt;
  &lt;li&gt;No sabes quién está del otro lado ni eres quien el otro cree que eres. Partiendo de esa base, trata a todos por igual.&lt;/li&gt;
  &lt;li&gt;Muchos sitios te muestran lo que ellos quieren que veas. Trata de que esto pase lo menos posible.&lt;/li&gt;
  &lt;li&gt;La pornografía legal es casi inevitable, pero ten en cuenta que es igual de cuestionable ver imágenes y noticias explícitas de muerte, corrupción y guerras alrededor del mundo.&lt;/li&gt;
  &lt;li&gt;Enséñales y enséñate como funciona internet. Tener una vaga idea de las piezas que la componen, se considerará en breve tiempo tan importante como saber leer y escribir.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;donde-puedo-empezar&quot;&gt;¿Donde puedo empezar?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.vialibre.org.ar/&quot;&gt;Fundación Via Libre&lt;/a&gt; es conocida en Argentina por promover el conocimiento libre, te recomiendo pegarte una vuelta al sitio.&lt;/p&gt;

&lt;p&gt;También puedes empezar leyendo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.vialibre.org.ar/2013/07/17/programas-para-proteger-la-privacidad-en-la-red/&quot;&gt;Programas para proteger la privacidad en la red&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://es.wikipedia.org/wiki/Privacidad_en_Internet&quot;&gt;Privacidad en internet&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://es.wikibooks.org/wiki/Anonimato_y_privacidad_en_internet&quot;&gt;Anonimato y privacidad en internet&lt;/a&gt;. Este es un “wikilibro” y aún está en desarrollo, pero es un buen punto de partida.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>https://github.com/3manuek/3manuek.github.io.git/anonimato</link>
                <guid>https://github.com/3manuek/3manuek.github.io.git/anonimato</guid>
                <pubDate>Fri, 03 Apr 2015 00:00:00 -0300</pubDate>
        </item>


</channel>
</rss>
